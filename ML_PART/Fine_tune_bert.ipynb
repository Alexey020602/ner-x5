{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0jU5kuXfijpT","executionInfo":{"status":"ok","timestamp":1759179235926,"user_tz":-180,"elapsed":6485,"user":{"displayName":"Александр Федоров","userId":"11558309455709398811"}},"outputId":"66e91234-c998-40d7-edd0-8a339ea89998"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/MyDrive/Colab Notebooks\n","Введи GitHub PAT токен: ··········\n","Заново склонировали репу\n","fatal: destination path 'entities-extraction-x5' already exists and is not an empty directory.\n","/content/drive/MyDrive/Colab Notebooks/entities-extraction-x5/ML_PART\n","✅ Всё готово! Рабочая папка: /content/drive/MyDrive/Colab Notebooks/entities-extraction-x5/ML_PART\n"]}],"source":["from google.colab import drive\n","import getpass, os\n","\n","# === Настройка проекта ===\n","USER = \"tokarevdr\"   # твой GitHub username\n","REPO = \"entities-extraction-x5\"            # название репозитория\n","EMAIL = \"fedorov.alexander.04@gmail.com\"    # твоя почта для git\n","NAME = \"Alexander\"           # твоё имя для git\n","# === Подключение Google Drive ===\n","drive.mount('/content/drive')\n","PROJECTS_DIR = \"/content/drive/MyDrive/Colab Notebooks\"\n","%cd $PROJECTS_DIR\n","# === GitHub авторизация ===\n","token = getpass.getpass('Введи GitHub PAT токен: ')\n","os.environ[\"GITHUB_TOKEN\"] = token\n","\n","\n","# === Проверяем: если репозиторий ещё не скачан, клонируем ===\n","if not os.path.exists(f\"{PROJECTS_DIR}/{REPO}/ML PART\"):\n","    print('Заново склонировали репу')\n","    !git clone https://{USER}:{os.environ[\"GITHUB_TOKEN\"]}@github.com/{USER}/{REPO}.git\n","# === Переходим в папку проекта ===\n","%cd {REPO}/{'ML_PART'}\n","\n","# === Настройка Git ===\n","!git config --global user.email \"{EMAIL}\"\n","!git config --global user.name \"{NAME}\"\n","!git remote set-url origin https://{USER}:{os.environ[\"GITHUB_TOKEN\"]}@github.com/{USER}/{REPO}.git\n","\n","print(\"✅ Всё готово! Рабочая папка:\", os.getcwd())\n"]},{"cell_type":"code","source":[],"metadata":{"id":"_69974-nHwLK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Установка зависимостей\n","!pip install -r requirements_bert.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"43TOp5KPoqEB","executionInfo":{"status":"ok","timestamp":1759180014588,"user_tz":-180,"elapsed":19465,"user":{"displayName":"Александр Федоров","userId":"11558309455709398811"}},"outputId":"709889d4-b3a9-455f-b0f5-1cc842e5c67c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch==2.8.0 in /usr/local/lib/python3.12/dist-packages (from -r requirements_bert.txt (line 2)) (2.8.0+cu126)\n","Collecting transformers==4.35.2 (from -r requirements_bert.txt (line 3))\n","  Downloading transformers-4.35.2-py3-none-any.whl.metadata (123 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.5/123.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting huggingface_hub==0.20.3 (from -r requirements_bert.txt (line 6))\n","  Downloading huggingface_hub-0.20.3-py3-none-any.whl.metadata (12 kB)\n","Collecting accelerate==0.24.1 (from -r requirements_bert.txt (line 7))\n","  Downloading accelerate-0.24.1-py3-none-any.whl.metadata (18 kB)\n","Collecting tokenizers==0.15.0 (from -r requirements_bert.txt (line 8))\n","  Downloading tokenizers-0.15.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n","Collecting datasets==2.14.7 (from -r requirements_bert.txt (line 9))\n","  Downloading datasets-2.14.7-py3-none-any.whl.metadata (19 kB)\n","Collecting pandas==2.1.4 (from -r requirements_bert.txt (line 12))\n","  Downloading pandas-2.1.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n","Collecting numpy==1.24.3 (from -r requirements_bert.txt (line 13))\n","  Downloading numpy-1.24.3.tar.gz (10.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m82.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","  \n","  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n","  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","  \u001b[31m╰─>\u001b[0m See above for output.\n","  \n","  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25herror\n","\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","\n","\u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n","\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","\u001b[31m╰─>\u001b[0m See above for output.\n","\n","\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n"]}]},{"cell_type":"code","source":["! pip install --upgrade onnxruntime"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WKM-_g4rH0SC","executionInfo":{"status":"ok","timestamp":1759180063037,"user_tz":-180,"elapsed":6747,"user":{"displayName":"Александр Федоров","userId":"11558309455709398811"}},"outputId":"17a53c7c-ddf7-491d-b032-759ebe779a45"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: onnxruntime in /usr/local/lib/python3.12/dist-packages (1.23.0)\n","Requirement already satisfied: coloredlogs in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (15.0.1)\n","Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (25.2.10)\n","Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (2.0.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (25.0)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (5.29.5)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (1.13.3)\n","Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.12/dist-packages (from coloredlogs->onnxruntime) (10.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime) (1.3.0)\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","import getpass, os, json, random, time\n","import numpy as np\n","import pandas as pd\n","import torch\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split, KFold\n","from torch.utils.data import Dataset, DataLoader\n","from torch.optim import AdamW\n","\n","# Импорты transformers с обработкой ошибок\n","try:\n","    from transformers import AutoTokenizer, AutoModelForTokenClassification, get_scheduler\n","    print(\"✅ Transformers успешно импортированы\")\n","except ImportError as e:\n","    print(f\"❌ Ошибка импорта transformers: {e}\")\n","    !pip install transformers==4.35.2\n","    from transformers import AutoTokenizer, AutoModelForTokenClassification, get_scheduler\n","\n","try:\n","    from TorchCRF import CRF\n","    print(\"✅ TorchCRF успешно импортирован\")\n","except ImportError as e:\n","    print(f\"❌ Ошибка импорта TorchCRF: {e}\")\n","    !pip install torchcrf==1.2.0\n","    from TorchCRF import CRF\n","\n","import ast\n","import traceback\n","from module import calculate_ner_metrics, calculate_macro_f1, process_submission_bert, \\\n","                  setup_hf_login, save_bert_to_hf, load_bert_from_hf, list_my_repos, check_repo_exists\n","from torch.nn.utils.rnn import pad_sequence"],"metadata":{"id":"cU-8Ca9forrD","executionInfo":{"status":"error","timestamp":1759180097268,"user_tz":-180,"elapsed":34225,"user":{"displayName":"Александр Федоров","userId":"11558309455709398811"}},"colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"866e209d-e929-45d2-c07d-ec6804b918fb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["❌ Ошибка импорта transformers: cannot import name 'GenerationMixin' from 'transformers.generation' (/usr/local/lib/python3.12/dist-packages/transformers/generation/__init__.py)\n","Collecting transformers==4.35.2\n","  Using cached transformers-4.35.2-py3-none-any.whl.metadata (123 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.35.2) (3.19.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from transformers==4.35.2) (0.35.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.35.2) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.35.2) (25.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.35.2) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.35.2) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.35.2) (2.32.4)\n","Collecting tokenizers<0.19,>=0.14 (from transformers==4.35.2)\n","  Downloading tokenizers-0.15.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.35.2) (0.6.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.35.2) (4.67.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.35.2) (2025.3.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.35.2) (4.15.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.35.2) (1.1.10)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.35.2) (3.4.3)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.35.2) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.35.2) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.35.2) (2025.8.3)\n","Downloading transformers-4.35.2-py3-none-any.whl (7.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tokenizers-0.15.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: tokenizers, transformers\n","  Attempting uninstall: tokenizers\n","    Found existing installation: tokenizers 0.22.0\n","    Uninstalling tokenizers-0.22.0:\n","      Successfully uninstalled tokenizers-0.22.0\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.56.1\n","    Uninstalling transformers-4.56.1:\n","      Successfully uninstalled transformers-4.56.1\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","sentence-transformers 5.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.35.2 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed tokenizers-0.15.2 transformers-4.35.2\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["tokenizers","transformers"]},"id":"b216cd1509df4de98d18b7ae936757e1"}},"metadata":{}},{"output_type":"error","ename":"ImportError","evalue":"cannot import name 'is_torch_tpu_available' from 'transformers.utils' (/usr/local/lib/python3.12/dist-packages/transformers/utils/__init__.py)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3563112187.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoModelForTokenClassification\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_scheduler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"✅ Transformers успешно импортированы\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mis_tokenizers_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0mtokenization_utils_fast\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPreTrainedTokenizerFast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mThis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mgeneric\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0minstantiated\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mone\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mlibrary\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mcreated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mthe\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mBaseAutoModelClass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mthe\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mBaseAutoModelClass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_config\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mclass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0mmethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'GenerationMixin' from 'transformers.generation' (/usr/local/lib/python3.12/dist-packages/transformers/generation/__init__.py)","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3563112187.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"❌ Ошибка импорта transformers: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install transformers==4.35.2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoModelForTokenClassification\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_scheduler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/optimization.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLambdaLR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mReduceLROnPlateau\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtrainer_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSchedulerType\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequire_version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m from .utils import (\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mExplicitEnum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mis_psutil_available\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'is_torch_tpu_available' from 'transformers.utils' (/usr/local/lib/python3.12/dist-packages/transformers/utils/__init__.py)","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"code","source":["# --- Основные пути для сохранения результатов ---\n","WHERE_DATA = 'cleared_data'\n","BASE_MODEL_NAME = \"bert\"\n","OUT_DIR = f\"OUTPUT/{WHERE_DATA}/{BASE_MODEL_NAME}\"\n","os.makedirs(OUT_DIR, exist_ok=True)\n","FINAL_METRICS_PATH = f\"{OUT_DIR}/final_training_metrics_per_epoch.csv\"\n","MODEL_PATH = f'MODELS/{WHERE_DATA}/{BASE_MODEL_NAME}'\n","os.makedirs(MODEL_PATH, exist_ok=True)\n","DATA_DIR = f'data/{WHERE_DATA}/'\n","PATIENCE = 2\n","SEED = 42\n"],"metadata":{"id":"bO4KcUNr0ASv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Hugging Face настройки\n","HF_TOKEN= getpass.getpass('Введи HFT токен: ')\n","HF_USERNAME = \"alexflex04\"\n","BERT_REPO_NAME = f\"{HF_USERNAME}/NER_{WHERE_DATA}_bert\"\n","\n","setup_hf_login(HF_TOKEN)"],"metadata":{"id":"0KGUxpTbJQSH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""],"metadata":{"id":"t_VWEzvBF9nH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["random.seed(SEED)\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","torch.cuda.manual_seed_all(SEED)\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False"],"metadata":{"id":"mFL3k8dl0FOh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Используемое устройство: {device}\")"],"metadata":{"id":"jgkUd-P9ov1X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["CONFIG = {\n","    \"model_checkpoint\": \"DeepPavlov/rubert-base-cased\",\n","    \"num_epochs\": 20,\n","    \"batch_size\": 128,\n","    \"learning_rate\": 2e-5,\n","    \"weight_decay\": 0.01,\n","    \"patience\": PATIENCE,\n","    \"label_list\": [\"O\", \"B-TYPE\", \"I-TYPE\", \"B-BRAND\", \"I-BRAND\", \"B-VOLUME\", \"I-VOLUME\", \"B-PERCENT\", \"I-PERCENT\"],\n","    \"id2label\": {i: label for i, label in enumerate([\"O\", \"B-TYPE\", \"I-TYPE\", \"B-BRAND\", \"I-BRAND\", \"B-VOLUME\", \"I-VOLUME\", \"B-PERCENT\", \"I-PERCENT\"])},\n","    \"label2id\": {label: i for i, label in enumerate([\"O\", \"B-TYPE\", \"I-TYPE\", \"B-BRAND\", \"I-BRAND\", \"B-VOLUME\", \"I-VOLUME\", \"B-PERCENT\", \"I-PERCENT\"])},\n","    \"metrics_csv\": f\"{OUT_DIR}/screening_metrics.csv\",\n","    \"submission_input\": f\"{DATA_DIR}/submission.csv\",\n","    \"submission_output\": f\"{OUT_DIR}/submission_response_bert.csv\"\n","}\n"],"metadata":{"id":"j_L-1esl0O58"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Загрузка данных\n","train_split = pd.read_csv(f\"{DATA_DIR}train.csv\")\n","valid_data = pd.read_csv(f\"{DATA_DIR}val.csv\")\n","\n","def parse_row_to_example(row):\n","    try:\n","        ann = ast.literal_eval(row['annotation'])\n","    except Exception:\n","        ann = []\n","    return (row['sample'], {'entities': ann})\n","\n","train_data = [parse_row_to_example(row) for _, row in train_split.iterrows()]\n","valid_data = [parse_row_to_example(row) for _, row in valid_data.iterrows()]\n","\n","tokenizer = AutoTokenizer.from_pretrained(CONFIG[\"model_checkpoint\"])"],"metadata":{"id":"4j7jeRhb0Zfz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def _normalize_entity_tuple(entity):\n","    if isinstance(entity, dict) and all(k in entity for k in (\"start\", \"end\", \"label\")):\n","        return int(entity[\"start\"]), int(entity[\"end\"]), str(entity[\"label\"])\n","    if isinstance(entity, (list, tuple)) and len(entity) == 3:\n","        a, b, c = entity\n","        if isinstance(a, int) and isinstance(b, int):\n","            return int(a), int(b), str(c)\n","        if isinstance(a, str) and isinstance(b, int) and isinstance(c, int):\n","            return int(b), int(c), str(a)\n","    raise ValueError(f\"Unknown entity format: {entity}\")\n","\n","def spans_to_bio(text, entities, tokenizer):\n","    tokenized = tokenizer(text, return_offsets_mapping=True, truncation=True, max_length=512)\n","    offsets = tokenized[\"offset_mapping\"]\n","    input_ids = tokenized[\"input_ids\"]\n","    labels = [CONFIG[\"label2id\"][\"O\"]] * len(input_ids)\n","\n","    valid_spans = []\n","    for ent in entities:\n","        try:\n","            s, e, l = _normalize_entity_tuple(ent)\n","        except Exception as ex:\n","            continue\n","\n","        if s < 0 or e < 0 or s >= e or e > len(text):\n","            continue\n","\n","        if l == \"O\":\n","            continue\n","\n","        valid_spans.append((s, e, l))\n","\n","    valid_spans.sort(key=lambda x: (x[0], -x[1]))\n","    merged = []\n","    for s, e, l in valid_spans:\n","        if merged and s < merged[-1][1]:\n","            prev_s, prev_e, prev_l = merged[-1]\n","            if (e - s) > (prev_e - prev_s):\n","                merged[-1] = (s, e, l)\n","        else:\n","            merged.append((s, e, l))\n","\n","    for s, e, l in merged:\n","        if l.startswith((\"B-\", \"I-\")):\n","            base = l.split(\"-\", 1)[1]\n","        else:\n","            base = l\n","\n","        b_tag = f\"B-{base}\"\n","        i_tag = f\"I-{base}\"\n","\n","        if b_tag not in CONFIG[\"label2id\"]:\n","            continue\n","\n","        b_id = CONFIG[\"label2id\"][b_tag]\n","        i_id = CONFIG[\"label2id\"][i_tag]\n","\n","        token_indices = []\n","        for idx, (tok_start, tok_end) in enumerate(offsets):\n","            if tok_start == tok_end:\n","                continue\n","            if max(tok_start, s) < min(tok_end, e):\n","                token_indices.append(idx)\n","\n","        if not token_indices:\n","            continue\n","\n","        for i, idx in enumerate(token_indices):\n","            if i == 0:\n","                labels[idx] = b_id\n","            else:\n","                labels[idx] = i_id\n","\n","    return tokenized[\"input_ids\"], tokenized[\"attention_mask\"], labels\n","\n","def bio_to_spans(text, bio_labels, offsets):\n","    entities = []\n","    current_entity = None\n","\n","    for i, (label_id, (start, end)) in enumerate(zip(bio_labels, offsets)):\n","        if start == end:\n","            continue\n","\n","        label = CONFIG[\"id2label\"][int(label_id)]\n","\n","        if label == \"O\":\n","            if current_entity is not None:\n","                entities.append(current_entity)\n","                current_entity = None\n","            continue\n","\n","        if label.startswith(\"B-\"):\n","            if current_entity is not None:\n","                entities.append(current_entity)\n","            entity_type = label[2:]\n","            current_entity = (start, end, entity_type)\n","\n","        elif label.startswith(\"I-\"):\n","            entity_type = label[2:]\n","            if current_entity is not None and current_entity[2] == entity_type:\n","                current_entity = (current_entity[0], end, entity_type)\n","            else:\n","                if current_entity is not None:\n","                    entities.append(current_entity)\n","                current_entity = (start, end, entity_type)\n","\n","    if current_entity is not None:\n","        entities.append(current_entity)\n","\n","    return entities"],"metadata":{"id":"ge5Fxkel0aFT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"\\n=== ПРОВЕРКА spans_to_bio / bio_to_spans ===\")\n","sample_text = \"молоко Простоквашино 2.5% 1 л\"\n","test_entities = [\n","    (0, 6, \"TYPE\"),\n","    {\"start\": 7, \"end\": 19, \"label\": \"BRAND\"},\n","    (21, 25, \"PERCENT\"),\n","    (26, 29, \"VOLUME\"),\n","    (30, 10, \"TYPE\"),        # некорректный\n","    (0, 1000, \"TYPE\")        # вне границ\n","]\n","ids, mask, labs = spans_to_bio(sample_text, test_entities, tokenizer)\n","print(\"Токены:\", tokenizer.convert_ids_to_tokens(ids))\n","print(\"BIO:\", [CONFIG[\"id2label\"][x] for x in labs])\n","offs = tokenizer(sample_text, return_offsets_mapping=True)[\"offset_mapping\"]\n","print(\"Назад в спаны:\", bio_to_spans(sample_text, labs, offs))\n","\n","# Пересечения\n","print(\"\\n=== Проверка пересекающихся спанов ===\")\n","text2 = \"чипсы лейс краб\"\n","ents2 = [(0, 5, \"TYPE\"), (4, 9, \"BRAND\")]\n","_, _, labs2 = spans_to_bio(text2, ents2, tokenizer)\n","print(\"BIO:\", [CONFIG[\"id2label\"][x] for x in labs2])\n","print(\"Назад в спаны:\", bio_to_spans(text2, labs2, tokenizer(text2, return_offsets_mapping=True)[\"offset_mapping\"]))\n"],"metadata":{"id":"o-hGbaZA0cMk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class NERDataset(Dataset):\n","    def __init__(self, data, tokenizer):\n","        self.data = data\n","        self.tokenizer = tokenizer\n","        self.processed_data = []\n","\n","        for text, ann in data:\n","            try:\n","                input_ids, attention_mask, labels = spans_to_bio(text, ann['entities'], tokenizer)\n","                self.processed_data.append({\n","                    \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n","                    \"attention_mask\": torch.tensor(attention_mask, dtype=torch.long),\n","                    \"labels\": torch.tensor(labels, dtype=torch.long)\n","                })\n","            except Exception as e:\n","                continue\n","\n","    def __len__(self):\n","        return len(self.processed_data)\n","\n","    def __getitem__(self, idx):\n","        return self.processed_data[idx]\n","\n","def collate_fn(batch):\n","    input_ids = [item[\"input_ids\"] for item in batch]\n","    attention_mask = [item[\"attention_mask\"] for item in batch]\n","    labels = [item[\"labels\"] for item in batch]\n","\n","    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=0)\n","    attention_mask = pad_sequence(attention_mask, batch_first=True, padding_value=0)\n","    labels = pad_sequence(labels, batch_first=True, padding_value=0)\n","\n","    return {\n","        \"input_ids\": input_ids,\n","        \"attention_mask\": attention_mask,\n","        \"labels\": labels\n","    }\n","\n","class NERModelWithCRF(torch.nn.Module):\n","    def __init__(self, num_labels):\n","        super().__init__()\n","        self.bert = AutoModelForTokenClassification.from_pretrained(CONFIG[\"model_checkpoint\"], num_labels=num_labels)\n","        self.crf = CRF(num_labels)\n","\n","    def forward(self, input_ids, attention_mask, labels=None):\n","        outputs = self.bert(input_ids, attention_mask=attention_mask)\n","        emissions = outputs.logits\n","        if labels is not None:\n","            loss = -self.crf(emissions, labels, mask=attention_mask.type(torch.uint8))\n","            return loss\n","        else:\n","            return self.crf.decode(emissions, mask=attention_mask.type(torch.uint8))\n","\n","def evaluate_model(model, eval_data, tokenizer):\n","    model.eval()\n","    entity_pairs = []\n","    for text, annotations in eval_data:\n","        tokenized = tokenizer([text], padding=True, truncation=True, return_tensors=\"pt\", return_offsets_mapping=True)\n","        input_ids = tokenized[\"input_ids\"].to(device)\n","        attention_mask = tokenized[\"attention_mask\"].to(device)\n","        with torch.no_grad():\n","            pred = model(input_ids, attention_mask)[0]\n","        offsets = tokenized[\"offset_mapping\"][0].tolist()\n","        pred_spans = bio_to_spans(text, pred, offsets)\n","        true_entities = annotations['entities']\n","        entity_pairs.append((true_entities, pred_spans))\n","\n","    macro_f1, f1_type, f1_brand, f1_volume, f1_percent = calculate_macro_f1(entity_pairs)\n","    return {\n","        'f1_macro': macro_f1,\n","        'f1_TYPE': f1_type,\n","        'f1_BRAND': f1_brand,\n","        'f1_VOLUME': f1_volume,\n","        'f1_PERCENT': f1_percent\n","    }\n"],"metadata":{"id":"ViwlaKNrFIkC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ueU3QeiOcsC8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"=== ИНИЦИАЛИЗАЦИЯ МОДЕЛИ ===\")\n","model = NERModelWithCRF(len(CONFIG[\"label_list\"])).to(device)\n","optimizer = AdamW(model.parameters(), lr=CONFIG[\"learning_rate\"], weight_decay=CONFIG[\"weight_decay\"])\n","num_training_steps = CONFIG[\"num_epochs\"] * len(train_data) // CONFIG[\"batch_size\"]\n","scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n","\n","train_dataset = NERDataset(train_data, tokenizer)\n","train_loader = DataLoader(train_dataset, batch_size=CONFIG[\"batch_size\"], shuffle=True, collate_fn=collate_fn)\n","\n","valid_dataset = NERDataset(valid_data, tokenizer)\n","valid_loader = DataLoader(valid_dataset, batch_size=CONFIG[\"batch_size\"], shuffle=False, collate_fn=collate_fn)\n","\n","metrics_df = pd.DataFrame(columns=['epoch', 'loss', 'f1_macro', 'f1_TYPE', 'f1_BRAND', 'f1_VOLUME', 'f1_PERCENT'])\n","best_f1 = 0\n","patience_counter = 0\n","best_epoch = 0\n","\n","print(\"✅ Модель и данные успешно подготовлены!\")\n","print(f\"Размер обучающей выборки: {len(train_dataset)}\")\n","print(f\"Размер валидационной выборки: {len(valid_dataset)}\")"],"metadata":{"id":"92aearUZ0hEQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"\\n=== НАЧАЛО SCREENING ОБУЧЕНИЯ ===\")\n","try:\n","    for epoch in range(CONFIG[\"num_epochs\"]):\n","        model.train()\n","        total_loss = 0\n","        for batch in train_loader:\n","            input_ids = batch[\"input_ids\"].to(device)\n","            attention_mask = batch[\"attention_mask\"].to(device)\n","            labels = batch[\"labels\"].to(device)\n","            loss = model(input_ids, attention_mask, labels)\n","            total_loss += loss.item()\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","            scheduler.step()\n","\n","        avg_loss = total_loss / len(train_loader)\n","        eval_metrics = evaluate_model(model, valid_data, tokenizer)\n","        current_f1 = eval_metrics[\"f1_macro\"]\n","\n","        metrics_row = {\n","            'epoch': epoch + 1,\n","            'loss': avg_loss,\n","            **eval_metrics\n","        }\n","        metrics_df = pd.concat([metrics_df, pd.DataFrame([metrics_row])], ignore_index=True)\n","\n","        print(f'Эпоха {epoch + 1:<3} | Loss: {avg_loss:.4f} | '\n","              f'F1-macro: {current_f1:.4f} | '\n","              f'F1-TYPE: {eval_metrics[\"f1_TYPE\"]:.4f} | '\n","              f'F1-BRAND: {eval_metrics[\"f1_BRAND\"]:.4f} | '\n","              f'F1-VOLUME: {eval_metrics[\"f1_VOLUME\"]:.4f} | '\n","              f'F1-PERCENT: {eval_metrics[\"f1_PERCENT\"]:.4f}')\n","\n","        if current_f1 > best_f1:\n","            best_f1 = current_f1\n","            best_epoch = epoch + 1\n","            patience_counter = 0\n","        else:\n","            patience_counter += 1\n","            print(f\"⏳ Patience: {patience_counter}/{PATIENCE}\")\n","            if patience_counter >= PATIENCE:\n","                print(f\"\\n🛑 Ранняя остановка на эпохе {epoch + 1}\")\n","                print(f\"Лучший F1-macro: {best_f1:.4f} достигнут на эпохе {best_epoch}\")\n","                break\n","\n","except Exception as e:\n","    print(f'💥 Критическая ошибка: {str(e)}')\n","    print(traceback.format_exc())\n","\n","finally:\n","    # Сохранение screening модели на HF\n","    print(f\"\\n💾 Сохранение screening модели на HF: {BERT_REPO_NAME+'_screening'}\")\n","    success = save_bert_to_hf(model, tokenizer, CONFIG, BERT_REPO_NAME+'_screening', HF_TOKEN)\n","\n","    if success:\n","        print(f\"🎉 BERT screening модель успешно сохранена на HF: {BERT_REPO_NAME+'_screening'}\")\n","    else:\n","        print(\"❌ Не удалось сохранить BERT screening модель на HF\")\n","\n","    # Локальное сохранение\n","    # torch.save(model.state_dict(), f\"{MODEL_PATH}/model_screening.pt\")\n","    metrics_df.to_csv(CONFIG[\"metrics_csv\"], index=False)\n","    print(\"💾 Screening модель и метрики сохранены локально\")\n","\n","print(\"\\n\" + \"=\"*80)\n","print(\"ИТОГОВЫЕ РЕЗУЛЬТАТЫ SCREENING:\")\n","print(\"=\"*80)\n","print(f\"Лучший F1-macro: {best_f1:.4f} на эпохе {best_epoch}\")\n","print(f\"Всего эпох выполнено: {len(metrics_df)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":998},"id":"8IyWSwnO0jQ4","executionInfo":{"status":"error","timestamp":1759144612773,"user_tz":-180,"elapsed":709,"user":{"displayName":"TVLG LX6500","userId":"15785277625807876484"}},"outputId":"614cdb4a-cfec-42e1-be32-9cb70375d35a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["💥 Критическая ошибка: CUDA error: device-side assert triggered\n","Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n","\n","Traceback (most recent call last):\n","  File \"/tmp/ipython-input-668927516.py\", line 9, in <cell line: 0>\n","    loss = model(input_ids, attention_mask, labels)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n","    return forward_call(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/tmp/ipython-input-3522910203.py\", line 48, in forward\n","    loss = -self.crf(emissions, labels, mask=attention_mask.type(torch.uint8))\n","            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n","    return forward_call(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/TorchCRF/__init__.py\", line 49, in forward\n","    log_numerator = self._compute_numerator_log_likelihood(h, labels, mask)\n","                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/TorchCRF/__init__.py\", line 204, in _compute_numerator_log_likelihood\n","    [self._calc_trans_score_for_num_llh(\n","     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.12/dist-packages/TorchCRF/__init__.py\", line 253, in _calc_trans_score_for_num_llh\n","    trans_t = trans[y[:, t], y[:, t + 1]].squeeze(1)\n","              ~~~~~^^^^^^^^^^^^^^^^^^^^^^\n","torch.AcceleratorError: CUDA error: device-side assert triggered\n","Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n","\n","\n","⚠️ Обучение прервано из-за ошибки, сохранен текущий прогресс\n"]},{"output_type":"error","ename":"AcceleratorError","evalue":"CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAcceleratorError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-668927516.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# Сохранение финальной модели после скрининга\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# Вариант 1: Сохранение полной модели и BERT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{MODEL_PATH}/model_screening.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{MODEL_PATH}_screening_final\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m# Вариант 2: Экспорт в ONNX (закомментирован)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    965\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 967\u001b[0;31m             _save(\n\u001b[0m\u001b[1;32m    968\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    969\u001b[0m                 \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m   1264\u001b[0m                     \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_storage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1266\u001b[0;31m                     \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1267\u001b[0m             \u001b[0;31m# Now that it is on the CPU we can directly copy it into the zip file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m             \u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/storage.py\u001b[0m in \u001b[0;36mcpu\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;34m\"\"\"Return a CPU copy of this storage if it's not already on the CPU.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUntypedStorage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAcceleratorError\u001b[0m: CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"]}]},{"cell_type":"code","source":["# Визуализация\n","plt.figure(figsize=(15, 10))\n","plt.subplot(2, 1, 1)\n","plt.plot(metrics_df['epoch'], metrics_df['loss'], 'b-', linewidth=2, label='Loss')\n","plt.axvline(x=best_epoch, color='r', linestyle='--', alpha=0.7, label=f'Лучшая эпоха ({best_epoch})')\n","plt.xlabel('Эпоха')\n","plt.ylabel('Loss')\n","plt.title('Loss по эпохам')\n","plt.legend()\n","plt.grid(True, alpha=0.3)\n","\n","plt.subplot(2, 1, 2)\n","plt.plot(metrics_df['epoch'], metrics_df['f1_macro'], 'r-', linewidth=3, label='F1-macro')\n","plt.plot(metrics_df['epoch'], metrics_df['f1_TYPE'], 'g--', label='F1-TYPE')\n","plt.plot(metrics_df['epoch'], metrics_df['f1_BRAND'], 'b--', label='F1-BRAND')\n","plt.plot(metrics_df['epoch'], metrics_df['f1_VOLUME'], 'y--', label='F1-VOLUME')\n","plt.plot(metrics_df['epoch'], metrics_df['f1_PERCENT'], 'c--', label='F1-PERCENT')\n","plt.axvline(x=best_epoch, color='r', linestyle='--', alpha=0.7, label=f'Лучшая эпоха ({best_epoch})')\n","plt.xlabel('Эпоха')\n","plt.ylabel('F1 Score')\n","plt.title('F1 Scores по эпохам')\n","plt.legend()\n","plt.grid(True, alpha=0.3)\n","\n","plt.tight_layout()\n","plt.savefig(f\"{OUT_DIR}/screening_metrics.png\", dpi=300, bbox_inches='tight')\n","plt.show()"],"metadata":{"id":"a8NbgDCk0mEW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"\\n=== ПРОВЕРКА ЗАГРУЗКИ МОДЕЛИ ===\")\n","loaded_model, loaded_tokenizer, loaded_config = load_bert_from_hf(BERT_REPO_NAME+'_screening', HF_TOKEN, device)\n","\n","if loaded_model:\n","    print(\"✅ Модель успешно загружена с HF!\")\n","    test_text = \"молоко Простоквашино 2.5% 1л\"\n","    from module import HFWrapper\n","    wrapper = HFWrapper(loaded_model, loaded_tokenizer)\n","    doc = wrapper(test_text)\n","    entities = [(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]\n","    print(f\"Тестовый текст: '{test_text}'\")\n","    print(f\"Извлеченные сущности: {entities}\")\n","\n","    # Обработка submission файла\n","    print(f\"\\n=== ОБРАБОТКА SUBMISSION ФАЙЛА ===\")\n","    process_submission_bert(\n","        model=loaded_model,\n","        tokenizer=loaded_tokenizer,\n","        input_file=CONFIG[\"submission_input\"],\n","        output_file=f\"{OUT_DIR}/submission_screening.csv\"\n","    )\n","else:\n","    print(\"❌ Не удалось загрузить модель для тестирования\")"],"metadata":{"id":"oVXcgPQDKOoV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"cM7CtsGwKO5w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Ячейка 2: Подбор гиперпараметров (Tuning) с grid search\n","PARAM_GRID = {\n","    \"learning_rate\": [1e-5, 2e-5, 3e-5],\n","    \"batch_size\": [32, 64],\n","    \"epochs\": [10, 20],\n","    \"weight_decay\": [0.01, 0.1]\n","}\n","\n","grid_results = []\n","\n","for lr in PARAM_GRID[\"learning_rate\"]:\n","    for bsz in PARAM_GRID[\"batch_size\"]:\n","        for max_ep in PARAM_GRID[\"epochs\"]:\n","            for wd in PARAM_GRID[\"weight_decay\"]:\n","                combo = {\"learning_rate\": lr, \"batch_size\": bsz, \"epochs\": max_ep, \"weight_decay\": wd}\n","                print(f\"\\n=== Tuning combo: learning_rate={lr}, batch_size={bsz}, epochs={max_ep}, weight_decay={wd} ===\")\n","\n","                model = NERModelWithCRF(len(CONFIG[\"label_list\"])).to(device)\n","                optimizer = AdamW(model.parameters(), lr=lr, weight_decay=wd)\n","                num_training_steps = max_ep * len(train_data) // bsz\n","                scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n","\n","                train_loader = DataLoader(train_dataset, batch_size=bsz, shuffle=True)\n","\n","                patience_counter, best_f1, best_metrics = 0, 0.0, None\n","                for epoch in range(1, max_ep + 1):\n","                    model.train()\n","                    total_loss = 0\n","                    for batch in train_loader:\n","                        input_ids = batch[\"input_ids\"].to(device)\n","                        attention_mask = batch[\"attention_mask\"].to(device)\n","                        labels = batch[\"labels\"].to(device)\n","                        loss = model(input_ids, attention_mask, labels)\n","                        total_loss += loss.item()\n","                        optimizer.zero_grad()\n","                        loss.backward()\n","                        optimizer.step()\n","                        scheduler.step()\n","\n","                    avg_loss = total_loss / len(train_loader)\n","                    metrics = evaluate_model(model, valid_data, tokenizer)\n","                    metrics[\"epoch\"] = epoch\n","                    metrics[\"loss\"] = avg_loss\n","                    current_f1 = metrics[\"f1_macro\"]\n","\n","                    print(f\"Ep {epoch} | Loss: {metrics['loss']:.4f} | F1-macro: {current_f1:.4f}\")\n","\n","                    if current_f1 > best_f1:\n","                        best_f1 = current_f1\n","                        best_metrics = metrics\n","                        patience_counter = 0\n","                    else:\n","                        patience_counter += 1\n","                        if patience_counter >= PATIENCE:\n","                            break\n","\n","                combo[\"best_f1_macro\"] = best_f1\n","                combo[\"best_metrics\"] = best_metrics\n","                grid_results.append(combo)\n","\n","# Выбор лучших параметров\n","best_combo = max(grid_results, key=lambda x: x[\"best_f1_macro\"])\n","print(\"\\nBest tuning params:\", best_combo)\n","\n","# Сохранение результатов\n","pd.DataFrame(grid_results).to_csv(f\"{OUT_DIR}/tuning_summary.csv\", index=False)\n","with open(f\"{OUT_DIR}/tuning_detailed.json\", \"w\", encoding=\"utf-8\") as f:\n","    json.dump(grid_results, f, ensure_ascii=False, indent=2)\n","with open(f\"{OUT_DIR}/best_combo.json\", \"w\", encoding=\"utf-8\") as f:\n","    json.dump({k: v for k, v in best_combo.items() if k != \"best_metrics\"}, f, ensure_ascii=False, indent=2)\n","print(\"💾 Tuning результаты и best_combo сохранены\")\n"],"metadata":{"id":"6vKo5oFn0s2V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Ячейка 3: Кросс-валидация (CV) с лучшими параметрами\n","with open(f\"{OUT_DIR}/best_combo.json\", \"r\", encoding=\"utf-8\") as f:\n","    best_combo = json.load(f)\n","\n","best_lr = best_combo[\"learning_rate\"]\n","best_bsz = best_combo[\"batch_size\"]\n","best_max_ep = best_combo[\"epochs\"]\n","best_wd = best_combo[\"weight_decay\"]\n","\n","kf = KFold(n_splits=5, shuffle=True, random_state=SEED)\n","cv_results = []\n","fold_best_f1s = []\n","\n","for fold, (tr_idx, val_idx) in enumerate(kf.split(train_data), 1):\n","    print(f\"\\n=== CV Fold {fold} ===\")\n","    fold_train = [train_data[i] for i in tr_idx]\n","    fold_valid = [train_data[i] for i in val_idx]\n","\n","    fold_train_dataset = NERDataset(fold_train, tokenizer)\n","    fold_train_loader = DataLoader(fold_train_dataset, batch_size=best_bsz, shuffle=True)\n","\n","    model = NERModelWithCRF(len(CONFIG[\"label_list\"])).to(device)\n","    optimizer = AdamW(model.parameters(), lr=best_lr, weight_decay=best_wd)\n","    num_training_steps = best_max_ep * len(fold_train) // best_bsz\n","    scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n","\n","    patience_counter, best_f1, best_metrics = 0, 0.0, None\n","    for epoch in range(1, best_max_ep + 1):\n","        model.train()\n","        total_loss = 0\n","        for batch in fold_train_loader:\n","            input_ids = batch[\"input_ids\"].to(device)\n","            attention_mask = batch[\"attention_mask\"].to(device)\n","            labels = batch[\"labels\"].to(device)\n","            loss = model(input_ids, attention_mask, labels)\n","            total_loss += loss.item()\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","            scheduler.step()\n","\n","        avg_loss = total_loss / len(fold_train_loader)\n","        metrics = evaluate_model(model, fold_valid, tokenizer)\n","        metrics[\"epoch\"] = epoch\n","        metrics[\"loss\"] = avg_loss\n","        current_f1 = metrics[\"f1_macro\"]\n","\n","        print(f\"Fold {fold} Ep {epoch} | Loss: {metrics['loss']:.4f} | F1-macro: {current_f1:.4f}\")\n","\n","        if current_f1 > best_f1:\n","            best_f1 = current_f1\n","            best_metrics = metrics\n","            patience_counter = 0\n","        else:\n","            patience_counter += 1\n","            if patience_counter >= PATIENCE:\n","                break\n","\n","    cv_results.append({\"fold\": fold, \"best_f1_macro\": best_f1, \"best_metrics\": best_metrics})\n","    fold_best_f1s.append(best_f1)\n","\n","mean_f1 = np.mean(fold_best_f1s)\n","std_f1 = np.std(fold_best_f1s)\n","print(f\"\\nCV Results: Mean F1_macro = {mean_f1:.4f} ± {std_f1:.4f}\")\n","\n","# Сохранение результатов CV\n","pd.DataFrame(cv_results).to_csv(f\"{OUT_DIR}/cv_summary.csv\", index=False)\n","with open(f\"{OUT_DIR}/cv_detailed.json\", \"w\", encoding=\"utf-8\") as f:\n","    json.dump(cv_results, f, ensure_ascii=False, indent=2)\n","print(\"💾 CV результаты сохранены\")"],"metadata":{"id":"gU0AhvFT0tZq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Ячейка 4: Финальное обучение на объединённом датасете (train+val)\n","train_val = [(row['sample'], {'entities': ast.literal_eval(row['annotation'])}) for _, row in pd.concat([train_split, valid_data]).iterrows()]\n","train_val_dataset = NERDataset(train_val, tokenizer)\n","train_val_loader = DataLoader(train_val_dataset, batch_size=best_bsz, shuffle=True)\n","\n","model = NERModelWithCRF(len(CONFIG[\"label_list\"])).to(device)\n","optimizer = AdamW(model.parameters(), lr=best_lr, weight_decay=best_wd)\n","num_training_steps = best_max_ep * len(train_val) // best_bsz\n","scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n","\n","records = []\n","best_final_f1, patience_counter = 0.0, 0\n","for epoch in range(1, best_max_ep + 1):\n","    model.train()\n","    total_loss = 0\n","    for batch in train_val_loader:\n","        input_ids = batch[\"input_ids\"].to(device)\n","        attention_mask = batch[\"attention_mask\"].to(device)\n","        labels = batch[\"labels\"].to(device)\n","        loss = model(input_ids, attention_mask, labels)\n","        total_loss += loss.item()\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        scheduler.step()\n","\n","    avg_loss = total_loss / len(train_val_loader)\n","    print(f\"Эпоха {epoch} | Loss: {avg_loss:.4f}\")\n","\n","# Сохранение screening модели на HF\n","    print(f\"\\n💾 Сохранение screening модели на HF: {BERT_REPO_NAME}\")\n","    success = save_bert_to_hf(model, tokenizer, CONFIG, BERT_REPO_NAME, HF_TOKEN)\n","\n","    if success:\n","        print(f\"🎉 BERT screening модель успешно сохранена на HF: {BERT_REPO_NAME}\")\n","    else:\n","        print(\"❌ Не удалось сохранить BERT screening модель на HF\")\n","# Вариант 1: Сохранение BERT-модели\n","# model.bert.save_pretrained(MODEL_PATH)\n","# Вариант 2: Экспорт в ONNX (закомментирован)\n","# dummy_input_ids = torch.randint(0, tokenizer.vocab_size, (1, 512)).to(device)\n","# dummy_attention_mask = torch.ones(1, 512).to(device)\n","# torch.onnx.export(model.bert, (dummy_input_ids, dummy_attention_mask),\n","#                   CONFIG[\"onnx_model_path\"],\n","#                   export_params=True,\n","#                   opset_version=14,  # Изменено на 14\n","#                   input_names=['input_ids', 'attention_mask'],\n","#                   output_names=['logits'],\n","#                   dynamic_axes={'input_ids': {0: 'batch', 1: 'seq'},\n","#                                 'attention_mask': {0: 'batch', 1: 'seq'},\n","#                                 'logits': {0: 'batch', 1: 'seq'}})\n","# quantize_dynamic(CONFIG[\"onnx_model_path\"], CONFIG[\"quantized_onnx_path\"], weight_type=QuantType.QUInt8)\n","# print(f\"\\nFinal model saved: {MODEL_PATH}\")"],"metadata":{"id":"D7b_w0Nd0v2i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Wz7v_fJW0ycg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"\\n=== ПРОВЕРКА ЗАГРУЗКИ МОДЕЛИ ===\")\n","loaded_model, loaded_tokenizer, loaded_config = load_bert_from_hf(BERT_REPO_NAME, HF_TOKEN, device)\n","\n","if loaded_model:\n","    print(\"✅ Модель успешно загружена с HF!\")\n","    test_text = \"молоко Простоквашино 2.5% 1л\"\n","    from module import HFWrapper\n","    wrapper = HFWrapper(loaded_model, loaded_tokenizer)\n","    doc = wrapper(test_text)\n","    entities = [(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]\n","    print(f\"Тестовый текст: '{test_text}'\")\n","    print(f\"Извлеченные сущности: {entities}\")\n","\n","    # Обработка submission файла\n","    print(f\"\\n=== ОБРАБОТКА SUBMISSION ФАЙЛА ===\")\n","    process_submission_bert(\n","        model=loaded_model,\n","        tokenizer=loaded_tokenizer,\n","        input_file=CONFIG[\"submission_input\"],\n","        output_file=f\"{OUT_DIR}/submission.csv\"\n","    )\n","else:\n","    print(\"❌ Не удалось загрузить модель для тестирования\")"],"metadata":{"id":"LZfZkz6oL3-x"},"execution_count":null,"outputs":[]}]}