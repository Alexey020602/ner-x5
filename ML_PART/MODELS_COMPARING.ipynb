{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"mYKianfIli_u","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1759108479240,"user_tz":-180,"elapsed":73823,"user":{"displayName":"TVLG LX6500","userId":"15785277625807876484"}},"outputId":"37d346a3-fa16-404b-d024-e6956787cd23"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/Colab Notebooks\n","Введи GitHub PAT токен: ··········\n","Заново склонировали репу\n","fatal: destination path 'entities-extraction-x5' already exists and is not an empty directory.\n","/content/drive/MyDrive/Colab Notebooks/entities-extraction-x5/ML_PART\n","✅ Всё готово! Рабочая папка: /content/drive/MyDrive/Colab Notebooks/entities-extraction-x5/ML_PART\n"]}],"source":["from google.colab import drive\n","import getpass, os\n","\n","# === Настройка проекта ===\n","USER = \"tokarevdr\"   # твой GitHub username\n","REPO = \"entities-extraction-x5\"            # название репозитория\n","EMAIL = \"fedorov.alexander.04@gmail.com\"    # твоя почта для git\n","NAME = \"Alexander\"           # твоё имя для git\n","# === Подключение Google Drive ===\n","drive.mount('/content/drive')\n","PROJECTS_DIR = \"/content/drive/MyDrive/Colab Notebooks\"\n","%cd $PROJECTS_DIR\n","# === GitHub авторизация ===\n","token = getpass.getpass('Введи GitHub PAT токен: ')\n","os.environ[\"GITHUB_TOKEN\"] = token\n","\n","# === Проверяем: если репозиторий ещё не скачан, клонируем ===\n","if not os.path.exists(f\"{PROJECTS_DIR}/{REPO}/ML PART\"):\n","    print('Заново склонировали репу')\n","    !git clone https://{USER}:{os.environ[\"GITHUB_TOKEN\"]}@github.com/{USER}/{REPO}.git\n","# === Переходим в папку проекта ===\n","%cd {REPO}/{'ML_PART'}\n","\n","# === Настройка Git ===\n","!git config --global user.email \"{EMAIL}\"\n","!git config --global user.name \"{NAME}\"\n","!git remote set-url origin https://{USER}:{os.environ[\"GITHUB_TOKEN\"]}@github.com/{USER}/{REPO}.git\n","\n","print(\"✅ Всё готово! Рабочая папка:\", os.getcwd())"]},{"cell_type":"code","source":["import os\n","import json\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import torch\n","from transformers import AutoTokenizer, AutoModelForTokenClassification\n","from TorchCRF import CRF\n","import onnxruntime as ort\n","import spacy\n","import time\n","import psutil\n","from module import process_submission, HFWrapper"],"metadata":{"id":"QoTH2g7EPaDA","executionInfo":{"status":"ok","timestamp":1759110065294,"user_tz":-180,"elapsed":294,"user":{"displayName":"TVLG LX6500","userId":"15785277625807876484"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# Конфигурация для BERT (аналогична fine_tune_bert.py)\n","CONFIG = {\n","    \"model_checkpoint\": \"DeepPavlov/rubert-base-cased\",\n","    \"label_list\": [\"O\", \"B-TYPE\", \"I-TYPE\", \"B-BRAND\", \"I-BRAND\", \"B-VOLUME\", \"I-VOLUME\", \"B-PERCENT\", \"I-PERCENT\"],\n","    \"id2label\": {i: label for i, label in enumerate([\"O\", \"B-TYPE\", \"I-TYPE\", \"B-BRAND\", \"I-BRAND\", \"B-VOLUME\", \"I-VOLUME\", \"B-PERCENT\", \"I-PERCENT\"])},\n","    \"label2id\": {label: i for i, label in enumerate([\"O\", \"B-TYPE\", \"I-TYPE\", \"B-BRAND\", \"I-BRAND\", \"B-VOLUME\", \"I-VOLUME\", \"B-PERCENT\", \"I-PERCENT\"])}\n","}\n","\n","# Функция для преобразования BIO в спаны (из fine_tune_bert.py)\n","def bio_to_spans(text, bio_labels, offsets):\n","    entities = []\n","    current_start = None\n","    current_label = None\n","    for i, (label_id, (start, end)) in enumerate(zip(bio_labels, offsets)):\n","        if label_id == 0 or start == end:  # O or special\n","            if current_start is not None:\n","                entities.append((current_start, end, current_label))\n","                current_start = None\n","            continue\n","        label = CONFIG[\"id2label\"][label_id]\n","        prefix, ent_type = label.split('-')\n","        if prefix == 'B':\n","            if current_start is not None:\n","                entities.append((current_start, start, current_label))\n","            current_start = start\n","            current_label = ent_type\n","        elif prefix == 'I' and current_label == ent_type:\n","            continue\n","        else:\n","            if current_start is not None:\n","                entities.append((current_start, start, current_label))\n","            current_start = None\n","    if current_start is not None:\n","        entities.append((current_start, len(text), current_label))\n","    return entities\n","\n","# Модель с CRF для BERT\n","class NERModelWithCRF(torch.nn.Module):\n","    def __init__(self, num_labels):\n","        super().__init__()\n","        self.bert = AutoModelForTokenClassification.from_pretrained(CONFIG[\"model_checkpoint\"], num_labels=num_labels)\n","        self.crf = CRF(num_labels)  # Без batch_first\n","\n","    def forward(self, input_ids, attention_mask, labels=None):\n","        outputs = self.bert(input_ids, attention_mask=attention_mask)\n","        emissions = outputs.logits\n","        if labels is not None:\n","            loss = -self.crf(emissions, labels, mask=attention_mask.type(torch.uint8))\n","            return loss\n","        else:\n","            return self.crf.decode(emissions, mask=attention_mask.type(torch.uint8))"],"metadata":{"id":"MEyV_CAQBZR6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plot_screening_metrics(model_path, dataset_path):\n","    \"\"\"\n","    Построение графиков и вывод метрик для скрининга.\n","\n","    Args:\n","        model_path (str): Путь к директории модели (например, 'MODELS/cleared_data/bert' или 'ru_core_news_lg').\n","        dataset_path (str): Путь к данным (например, 'data/cleared_data/').\n","    \"\"\"\n","    out_dir = f\"OUTPUT{dataset_path.replace('data', '')}/{os.path.basename(model_path)}\"\n","    metrics_file = f\"{out_dir}/screening_metrics.csv\"\n","\n","    if not os.path.exists(metrics_file):\n","        print(f\"Файл метрик {metrics_file} не найден.\")\n","        return\n","\n","    metrics_df = pd.read_csv(metrics_file)\n","    best_f1 = metrics_df['f1_macro'].max()\n","    best_epoch = metrics_df.loc[metrics_df['f1_macro'].idxmax(), 'epoch']\n","\n","    print(\"\\n=== Метрики скрининга ===\")\n","    print(f\"Лучший F1-macro: {best_f1:.4f} на эпохе {best_epoch}\")\n","    print(\"\\nДетальные метрики по эпохам:\")\n","    print(metrics_df.round(4))\n","\n","    plt.figure(figsize=(15, 10))\n","    # График Loss\n","    plt.subplot(2, 1, 1)\n","    plt.plot(metrics_df['epoch'], metrics_df['loss'], 'b-', linewidth=2, label='Loss')\n","    plt.axvline(x=best_epoch, color='r', linestyle='--', alpha=0.7, label=f'Лучшая эпоха ({best_epoch})')\n","    plt.xlabel('Эпоха')\n","    plt.ylabel('Loss')\n","    plt.title('Loss по эпохам')\n","    plt.legend()\n","    plt.grid(True, alpha=0.3)\n","\n","    # График F1 scores\n","    plt.subplot(2, 1, 2)\n","    plt.plot(metrics_df['epoch'], metrics_df['f1_macro'], 'r-', linewidth=3, label='F1-macro')\n","    plt.plot(metrics_df['epoch'], metrics_df['f1_TYPE'], 'g--', label='F1-TYPE')\n","    plt.plot(metrics_df['epoch'], metrics_df['f1_BRAND'], 'b--', label='F1-BRAND')\n","    plt.plot(metrics_df['epoch'], metrics_df['f1_VOLUME'], 'y--', label='F1-VOLUME')\n","    plt.plot(metrics_df['epoch'], metrics_df['f1_PERCENT'], 'c--', label='F1-PERCENT')\n","    plt.axvline(x=best_epoch, color='r', linestyle='--', alpha=0.7, label=f'Лучшая эпоха ({best_epoch})')\n","    plt.xlabel('Эпоха')\n","    plt.ylabel('F1 Score')\n","    plt.title('F1 Scores по эпохам')\n","    plt.legend()\n","    plt.grid(True, alpha=0.3)\n","\n","    plt.tight_layout()\n","    plt.savefig(f\"{out_dir}/screening_metrics_comparison.png\", dpi=300, bbox_inches='tight')\n","    plt.show()\n","\n","def plot_tuning_metrics(model_path, dataset_path):\n","    \"\"\"\n","    Вывод таблицы и графика лучших F1 для tuning.\n","\n","    Args:\n","        model_path (str): Путь к директории модели.\n","        dataset_path (str): Путь к данным.\n","    \"\"\"\n","    out_dir = f\"OUTPUT{dataset_path.replace('data', '')}/{os.path.basename(model_path)}\"\n","    tuning_file = f\"{out_dir}/tuning_summary.csv\"\n","\n","    if not os.path.exists(tuning_file):\n","        print(f\"Файл tuning {tuning_file} не найден.\")\n","        return\n","\n","    tuning_df = pd.read_csv(tuning_file)\n","    best_combo = tuning_df.loc[tuning_df['best_f1_macro'].idxmax()]\n","\n","    print(\"\\n=== Результаты подбора гиперпараметров ===\")\n","    print(f\"Лучшая комбинация: {best_combo.to_dict()}\")\n","    print(\"\\nВсе комбинации:\")\n","    print(tuning_df.round(4))\n","\n","    plt.figure(figsize=(10, 6))\n","    plt.bar(tuning_df.index, tuning_df['best_f1_macro'], color='skyblue')\n","    plt.xlabel('Комбинация гиперпараметров')\n","    plt.ylabel('F1-macro')\n","    plt.title('F1-macro для всех комбинаций гиперпараметров')\n","    plt.grid(True, alpha=0.3)\n","    plt.savefig(f\"{out_dir}/tuning_metrics_comparison.png\", dpi=300, bbox_inches='tight')\n","    plt.show()\n","\n","def plot_cv_metrics(model_path, dataset_path):\n","    \"\"\"\n","    Вывод метрик и графика для кросс-валидации.\n","\n","    Args:\n","        model_path (str): Путь к директории модели.\n","        dataset_path (str): Путь к данным.\n","    \"\"\"\n","    out_dir = f\"OUTPUT{dataset_path.replace('data', '')}/{os.path.basename(model_path)}\"\n","    cv_file = f\"{out_dir}/cv_summary.csv\"\n","\n","    if not os.path.exists(cv_file):\n","        print(f\"Файл CV {cv_file} не найден.\")\n","        return\n","\n","    cv_df = pd.read_csv(cv_file)\n","    mean_f1 = cv_df['best_f1_macro'].mean()\n","    std_f1 = cv_df['best_f1_macro'].std()\n","\n","    print(\"\\n=== Результаты кросс-валидации ===\")\n","    print(f\"Mean F1-macro: {mean_f1:.4f} ± {std_f1:.4f}\")\n","    print(\"\\nМетрики по фолдам:\")\n","    print(cv_df.round(4))\n","\n","    plt.figure(figsize=(10, 6))\n","    plt.bar(cv_df['fold'], cv_df['best_f1_macro'], color='lightgreen')\n","    plt.axhline(y=mean_f1, color='r', linestyle='--', label=f'Mean F1-macro: {mean_f1:.4f}')\n","    plt.xlabel('Фолд')\n","    plt.ylabel('F1-macro')\n","    plt.title('F1-macro по фолдам кросс-валидации')\n","    plt.legend()\n","    plt.grid(True, alpha=0.3)\n","    plt.savefig(f\"{out_dir}/cv_metrics_comparison.png\", dpi=300, bbox_inches='tight')\n","    plt.show()\n"],"metadata":{"id":"pSBHwOM5Bdkc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def process_submission_hf(model_path, input_file, output_file):\n","    \"\"\"\n","    Создание тестового датасета с использованием Hugging Face модели.\n","\n","    Args:\n","        model_path (str): Путь к модели (например, 'MODELS/cleared_data/bert_screening').\n","        input_file (str): Путь к входному CSV (например, 'data/cleared_data/submission.csv').\n","        output_file (str): Путь к выходному CSV.\n","    \"\"\"\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    tokenizer = AutoTokenizer.from_pretrained(CONFIG[\"model_checkpoint\"])\n","    bert_model = AutoModelForTokenClassification.from_pretrained(model_path).to(device)\n","    model = NERModelWithCRF(len(CONFIG[\"label_list\"])).to(device)\n","    model.bert = bert_model\n","    wrapped_model = HFWrapper(model, tokenizer)\n","    process_submission(wrapped_model, input_file, output_file)\n","    print(f\"Тестовый датасет сохранён: {output_file}\")\n","\n","def process_submission_onnx(onnx_path, input_file, output_file):\n","    \"\"\"\n","    Создание тестового датасета с использованием ONNX модели.\n","\n","    Args:\n","        onnx_path (str): Путь к ONNX файлу (например, 'OUTPUT/cleared_data/bert/model.onnx').\n","        input_file (str): Путь к входному CSV.\n","        output_file (str): Путь к выходному CSV.\n","    \"\"\"\n","    class ONNXWrapper:\n","        def __init__(self, onnx_path, tokenizer):\n","            self.ort_session = ort.InferenceSession(onnx_path, providers=['CUDAExecutionProvider' if torch.cuda.is_available() else 'CPUExecutionProvider'])\n","            self.tokenizer = tokenizer\n","\n","        def __call__(self, text):\n","            class Doc:\n","                def __init__(self, ents):\n","                    self.ents = ents\n","\n","            class Ent:\n","                def __init__(self, start, end, label):\n","                    self.start_char = start\n","                    self.end_char = end\n","                    self.label_ = label\n","\n","            tokenized = self.tokenizer([text], padding=True, truncation=True, return_tensors=\"np\", return_offsets_mapping=True)\n","            ort_inputs = {\"input_ids\": tokenized[\"input_ids\"], \"attention_mask\": tokenized[\"attention_mask\"]}\n","            logits = self.ort_session.run(None, ort_inputs)[0]\n","            pred_labels = np.argmax(logits, axis=-1)[0]\n","            spans = bio_to_spans(text, pred_labels, tokenized[\"offset_mapping\"][0].tolist())\n","            ents = [Ent(s, e, l) for s, e, l in spans]\n","            return Doc(ents)\n","\n","    tokenizer = AutoTokenizer.from_pretrained(CONFIG[\"model_checkpoint\"])\n","    wrapped_model = ONNXWrapper(onnx_path, tokenizer)\n","    process_submission(wrapped_model, input_file, output_file)\n","    print(f\"Тестовый датасет сохранён: {output_file}\")\n","\n","def process_submission_spacy(model_path, input_file, output_file):\n","    \"\"\"\n","    Создание тестового датасета с использованием SpaCy модели.\n","\n","    Args:\n","        model_path (str): Путь к модели (например, 'MODELS/cleared_data/ru_core_news_lg').\n","        input_file (str): Путь к входному CSV.\n","        output_file (str): Путь к выходному CSV.\n","    \"\"\"\n","    nlp = spacy.load(model_path)\n","    process_submission(nlp, input_file, output_file)\n","    print(f\"Тестовый датасет сохранён: {output_file}\")\n","\n","# 3. Функция для измерения производительности\n","\n","def measure_inference_performance(model_path, model_type, sample_text, batch_size=32):\n","    \"\"\"\n","    Измерение производительности модели в инференсе.\n","\n","    Args:\n","        model_path (str): Путь к модели или ONNX файлу.\n","        model_type (str): Тип модели ('hf', 'onnx', 'spacy').\n","        sample_text (str or list): Текст для одного примера или список текстов для батча.\n","        batch_size (int): Размер батча для измерения.\n","\n","    Returns:\n","        Dict: Метрики производительности (время, CPU/GPU, память).\n","    \"\"\"\n","    process = psutil.Process()\n","    start_memory = process.memory_info().rss / 1024**2  # MB\n","\n","    if model_type == 'hf':\n","        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        tokenizer = AutoTokenizer.from_pretrained(CONFIG[\"model_checkpoint\"])\n","        bert_model = AutoModelForTokenClassification.from_pretrained(model_path).to(device)\n","        model = NERModelWithCRF(len(CONFIG[\"label_list\"])).to(device)\n","        model.bert = bert_model\n","        wrapped_model = HFWrapper(model, tokenizer)\n","\n","        # Один пример\n","        start_time = time.time()\n","        _ = wrapped_model(sample_text if isinstance(sample_text, str) else sample_text[0])\n","        single_time = time.time() - start_time\n","\n","        # Батч\n","        texts = [sample_text] * batch_size if isinstance(sample_text, str) else sample_text[:batch_size]\n","        start_time = time.time()\n","        for text in texts:\n","            _ = wrapped_model(text)\n","        batch_time = time.time() - start_time\n","\n","    elif model_type == 'onnx':\n","        tokenizer = AutoTokenizer.from_pretrained(CONFIG[\"model_checkpoint\"])\n","        ort_session = ort.InferenceSession(model_path, providers=['CUDAExecutionProvider' if torch.cuda.is_available() else 'CPUExecutionProvider'])\n","\n","        # Один пример\n","        start_time = time.time()\n","        tokenized = tokenizer([sample_text if isinstance(sample_text, str) else sample_text[0]],\n","                            padding=True, truncation=True, return_tensors=\"np\")\n","        ort_inputs = {\"input_ids\": tokenized[\"input_ids\"], \"attention_mask\": tokenized[\"attention_mask\"]}\n","        _ = ort_session.run(None, ort_inputs)\n","        single_time = time.time() - start_time\n","\n","        # Батч\n","        texts = [sample_text] * batch_size if isinstance(sample_text, str) else sample_text[:batch_size]\n","        tokenized = tokenizer(texts, padding=True, truncation=True, return_tensors=\"np\")\n","        start_time = time.time()\n","        _ = ort_session.run(None, {\"input_ids\": tokenized[\"input_ids\"], \"attention_mask\": tokenized[\"attention_mask\"]})\n","        batch_time = time.time() - start_time\n","\n","    elif model_type == 'spacy':\n","        nlp = spacy.load(model_path)\n","\n","        # Один пример\n","        start_time = time.time()\n","        _ = nlp(sample_text if isinstance(sample_text, str) else sample_text[0])\n","        single_time = time.time() - start_time\n","\n","        # Батч\n","        texts = [sample_text] * batch_size if isinstance(sample_text, str) else sample_text[:batch_size]\n","        start_time = time.time()\n","        for text in texts:\n","            _ = nlp(text)\n","        batch_time = time.time() - start_time\n","\n","    else:\n","        raise ValueError(\"model_type должен быть 'hf', 'onnx' или 'spacy'\")\n","\n","    end_memory = process.memory_info().rss / 1024**2  # MB\n","    memory_used = end_memory - start_memory\n","    cpu_usage = psutil.cpu_percent(interval=None)\n","    gpu_usage = torch.cuda.memory_allocated() / 1024**2 if torch.cuda.is_available() else 0  # MB\n","\n","    return {\n","        \"single_inference_time_s\": single_time,\n","        \"batch_inference_time_s\": batch_time,\n","        \"batch_size\": batch_size,\n","        \"cpu_usage_percent\": cpu_usage,\n","        \"gpu_usage_mb\": gpu_usage,\n","        \"memory_used_mb\": memory_used\n","    }"],"metadata":{"id":"bHYqmWh1BgBx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# dataset_path = \"data/cleared_data/\"\n","# sample_text = \"чипсы русская картошка краб\"\n","\n","# # BERT: скрининг и финальная модель\n","# print(\"\\n=== BERT Screening ===\")\n","# plot_screening_metrics(\"MODELS/cleared_data/bert_screening\", dataset_path)\n","# plot_tuning_metrics(\"MODELS/cleared_data/bert\", dataset_path)\n","# plot_cv_metrics(\"MODELS/cleared_data/bert\", dataset_path)\n","# process_submission_hf(\"MODELS/cleared_data/bert_screening\", f\"{dataset_path}/submission.csv\", \"submission_hf_screening.csv\")\n","# print(\"\\nPerformance for BERT Screening:\")\n","# print(measure_inference_performance(\"MODELS/cleared_data/bert_screening\", \"hf\", sample_text))\n","\n","# print(\"\\n=== BERT Final ===\")\n","# process_submission_hf(\"MODELS/cleared_data/bert\", f\"{dataset_path}/submission.csv\", \"submission_hf_final.csv\")\n","# print(\"\\nPerformance for BERT Final:\")\n","# print(measure_inference_performance(\"MODELS/cleared_data/bert\", \"hf\", sample_text))\n","\n","# # ONNX\n","# print(\"\\n=== BERT ONNX ===\")\n","# process_submission_onnx(\"OUTPUT/cleared_data/bert/model.onnx\", f\"{dataset_path}/submission.csv\", \"submission_onnx.csv\")\n","# print(\"\\nPerformance for ONNX:\")\n","# print(measure_inference_performance(\"OUTPUT/cleared_data/bert/model.onnx\", \"onnx\", sample_text))\n","\n","# # SpaCy\n","# print(\"\\n=== SpaCy ===\")\n","# plot_screening_metrics(\"MODELS/cleared_data/ru_core_news_lg\", dataset_path)\n","# plot_tuning_metrics(\"MODELS/cleared_data/ru_core_news_lg\", dataset_path)\n","# plot_cv_metrics(\"MODELS/cleared_data/ru_core_news_lg\", dataset_path)\n","# process_submission_spacy(\"MODELS/cleared_data/ru_core_news_lg\", f\"{dataset_path}/submission.csv\", \"submission_spacy.csv\")\n","# print(\"\\nPerformance for SpaCy:\")\n","# print(measure_inference_performance(\"MODELS/cleared_data/ru_core_news_lg\", \"spacy\", sample_text))"],"metadata":{"id":"bSjup0W2BjkB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["entity_pairs = [\n","    (\n","        [(0, 7, 'B-TYPE')],  # авокадо\n","        [(0, 7, 'B-BRAND')]\n","    ),\n","    (\n","        [(0, 5, 'B-TYPE')],  # батат\n","        [(0, 5, 'O')]\n","    ),\n","    (\n","        [(0, 8, 'B-TYPE'), (9, 13, 'B-VOLUME'), (14, 20, 'B-BRAND'), (21, 28, 'I-BRAND')],\n","        [(0, 8, 'B-TYPE'), (9, 13, 'B-VOLUME'), (14, 20, 'B-BRAND'), (21, 28, 'I-BRAND')]\n","    )\n","]\n","\n","macro_f1, f1_type, f1_brand, f1_volume, f1_percent = calculate_macro_f1(entity_pairs)\n","print(f\"Macro-averaged F1-score: {macro_f1:.4f}\")\n","print(f\"F1 TYPE: {f1_type:.4f}\")\n","print(f\"F1 BRAND: {f1_brand:.4f}\")\n","print(f\"F1 VOLUME: {f1_volume:.4f}\")\n","print(f\"F1 PERCENT: {f1_percent:.4f}\")"],"metadata":{"id":"htb9SjDMdLMZ","executionInfo":{"status":"ok","timestamp":1759111232489,"user_tz":-180,"elapsed":58,"user":{"displayName":"TVLG LX6500","userId":"15785277625807876484"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"05291ef7-e8c9-4c75-d796-ace7fae36b1d"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Macro-averaged F1-score: 0.7222\n","F1 TYPE: 0.5000\n","F1 BRAND: 0.6667\n","F1 VOLUME: 1.0000\n","F1 PERCENT: 0.0000\n"]}]},{"cell_type":"code","source":["loaded_models = compare_model_dataset(\n","        model_paths=[f\"{BASE_DIR}/MODELS/cleared_data/ru_core_news_lg_screening\"],\n","        model_types=[\"spacy\"],\n","        dataset_names=[\"cleared_data\"]\n","    )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pvdnXFUuIZdf","executionInfo":{"status":"ok","timestamp":1759111234174,"user_tz":-180,"elapsed":588,"user":{"displayName":"TVLG LX6500","userId":"15785277625807876484"}},"outputId":"f6473854-6955-462c-d7d6-6cc7677e9104"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Loaded spacy model for cleared_data from /content/drive/MyDrive/Colab Notebooks/entities-extraction-x5/ML_PART/MODELS/cleared_data/ru_core_news_lg_screening\n"]}]},{"cell_type":"code","source":["# Ручной вызов функций\n","for model, tokenizer, model_type, dataset_name in loaded_models:\n","    # Загрузка метрик\n","    metrics_path = f\"{BASE_DIR}/OUTPUT/cleared_data/ru_core_news_lg/screening_metrics.csv\"\n","    try:\n","        metrics_df = pd.read_csv(metrics_path)\n","        print(f\"\\n=== Metrics for {model_type}_{dataset_name} ===\")\n","        print(metrics_df[['epoch', 'f1_macro', 'loss']].tail())\n","    except FileNotFoundError:\n","        print(f\"Metrics not found at {metrics_path}\")\n","        continue\n","\n","    # Построение графика\n","    if 'metrics_df' in locals():\n","        plot_metrics_per_epoch(\n","            metrics_dfs=[metrics_df],\n","            model_names=[model_type],\n","            dataset_names=[dataset_name],\n","            output_path=f\"{OUTPUT_DIR}/metrics_per_epoch_{model_type}_{dataset_name}.png\"\n","        )\n","        print(f\"Plot saved to {OUTPUT_DIR}/metrics_per_epoch_{model_type}_{dataset_name}.png\")\n","\n","    # Оценка (если есть test.csv)\n","    test_path = f\"{BASE_DIR}/data/cleared_data/test.csv\"\n","    test_data = load_test_data(test_path)\n","    if test_data:\n","        if model_type == 'spacy':\n","            metrics = evaluate_spacy(model, test_data)\n","        elif model_type == 'bert':\n","            metrics = evaluate_bert(model, tokenizer, test_data)\n","        elif model_type == 'frida':\n","            metrics = evaluate_frida(model, tokenizer, test_data)\n","        if metrics:\n","            print(f\"\\n=== Evaluation for {model_type}_{dataset_name} ===\")\n","            print(f\"F1_macro: {metrics['f1_macro']:.4f}, \"\n","                  f\"F1_TYPE: {metrics['f1_TYPE']:.4f}, F1_BRAND: {metrics['f1_BRAND']:.4f}, \"\n","                  f\"F1_VOLUME: {metrics['f1_VOLUME']:.4f}, F1_PERCENT: {metrics['f1_PERCENT']:.4f}\")\n","\n","    # Submission (если есть input.csv)\n","    input_path = f\"{BASE_DIR}/data/cleared_data/input.csv\"\n","    submission_output = f\"{OUTPUT_DIR}/submission_{model_type}_{dataset_name}.csv\"\n","    if os.path.exists(input_path):\n","        if model_type == 'spacy':\n","            process_submission_spacy(model, input_path, submission_output)\n","        elif model_type == 'bert':\n","            process_submission_bert(model, tokenizer, input_path, submission_output)\n","        elif model_type == 'frida':\n","            process_submission_frida(model, tokenizer, input_path, submission_output)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NS6uJ4GKM1J9","executionInfo":{"status":"ok","timestamp":1759111288935,"user_tz":-180,"elapsed":1262,"user":{"displayName":"TVLG LX6500","userId":"15785277625807876484"}},"outputId":"b707f73a-7ff5-46cb-dc89-316d0b97d9ad"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","=== Metrics for spacy_cleared_data ===\n","    epoch  f1_macro       loss\n","10     11  0.806042  2020.3744\n","11     12  0.901783  1803.6511\n","12     13  0.828672  1715.7328\n","13     14  0.861513  1545.9503\n","14     15  0.816769  1453.2258\n","Plot saved to /content/drive/MyDrive/Colab Notebooks/entities-extraction-x5/ML_PART/OUTPUT/metrics_per_epoch_spacy_cleared_data.png\n","Test data not found at /content/drive/MyDrive/Colab Notebooks/entities-extraction-x5/ML_PART/data/cleared_data/test.csv\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"w8UMQJ_QIsEU"},"execution_count":null,"outputs":[]}]}