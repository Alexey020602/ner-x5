{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNpl5BM3qXkcIouZQCXjc0T"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["В этом файле будут лежать моирезультаты исследования данных, мбчто-то еще"],"metadata":{"id":"mc0CDFy91-Gb"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"e9anxhVk16FA"},"outputs":[],"source":["from google.colab import drive\n","import getpass, os\n","\n","# === Настройка проекта ===\n","USER = \"tokarevdr\"   # твой GitHub username\n","REPO = \"entities-extraction-x5\"            # название репозитория\n","EMAIL = \"fedorov.alexander.04@gmail.com\"    # твоя почта для git\n","NAME = \"Alexander\"           # твоё имя для git\n","# === Подключение Google Drive ===\n","drive.mount('/content/drive')\n","PROJECTS_DIR = \"/content/drive/MyDrive/Colab Notebooks\"\n","%cd $PROJECTS_DIR\n","# === GitHub авторизация ===\n","token = getpass.getpass('Введи GitHub PAT токен: ')\n","os.environ[\"GITHUB_TOKEN\"] = token\n","\n","# === Проверяем: если репозиторий ещё не скачан, клонируем ===\n","if not os.path.exists(f\"{PROJECTS_DIR}/{REPO}/ML PART\"):\n","    print('Заново склонировали репу')\n","    !git clone https://{USER}:{os.environ[\"GITHUB_TOKEN\"]}@github.com/{USER}/{REPO}.git\n","# === Переходим в папку проекта ===\n","%cd {REPO}/{'ML_PART'}\n","\n","# === Настройка Git ===\n","!git config --global user.email \"{EMAIL}\"\n","!git config --global user.name \"{NAME}\"\n","!git remote set-url origin https://{USER}:{os.environ[\"GITHUB_TOKEN\"]}@github.com/{USER}/{REPO}.git\n","\n","print(\"✅ Всё готово! Рабочая папка:\", os.getcwd())\n"]},{"cell_type":"code","source":["# ! git lfs track \"ML_PART/MODELS/**\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QilVSAu29b8p","executionInfo":{"status":"ok","timestamp":1759012064214,"user_tz":-180,"elapsed":930,"user":{"displayName":"Александр Федоров","userId":"11558309455709398811"}},"outputId":"396eaab1-a584-4b56-c5ce-801267608388"},"execution_count":82,"outputs":[{"output_type":"stream","name":"stdout","text":["Tracking \"ML_PART/MODELS/**\"\n"]}]},{"cell_type":"code","source":["! git status"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yyAhFdyc2Ono","executionInfo":{"status":"ok","timestamp":1759012196884,"user_tz":-180,"elapsed":37693,"user":{"displayName":"Александр Федоров","userId":"11558309455709398811"}},"outputId":"3e09d26c-67de-426f-c42d-83bfe9005f5b"},"execution_count":84,"outputs":[{"output_type":"stream","name":"stdout","text":["Refresh index: 100% (50/50), done.\n","On branch NER_models\n","Your branch is up to date with 'origin/NER_models'.\n","\n","Changes to be committed:\n","  (use \"git restore --staged <file>...\" to unstage)\n","\t\u001b[32mnew file:   .gitattributes\u001b[m\n","\t\u001b[32mdeleted:    Data_explorer.ipynb\u001b[m\n","\t\u001b[32mdeleted:    Fine_tuning_ru_core_web_lg.ipynb\u001b[m\n","\t\u001b[32mnew file:   ML_PART/Data_explorer.ipynb\u001b[m\n","\t\u001b[32mnew file:   ML_PART/Fine_tune_bert.ipynb\u001b[m\n","\t\u001b[32mnew file:   ML_PART/Fine_tuning_ru_core_web_lg.ipynb\u001b[m\n","\t\u001b[32mnew file:   ML_PART/MODELS/custom_ru_core_news_lg_with_9_labels_50_epochs/attribute_ruler/patterns\u001b[m\n","\t\u001b[32mnew file:   ML_PART/MODELS/custom_ru_core_news_lg_with_9_labels_50_epochs/config.cfg\u001b[m\n","\t\u001b[32mnew file:   ML_PART/MODELS/custom_ru_core_news_lg_with_9_labels_50_epochs/meta.json\u001b[m\n","\t\u001b[32mnew file:   ML_PART/MODELS/custom_ru_core_news_lg_with_9_labels_50_epochs/morphologizer/cfg\u001b[m\n","\t\u001b[32mnew file:   ML_PART/MODELS/custom_ru_core_news_lg_with_9_labels_50_epochs/morphologizer/model\u001b[m\n","\t\u001b[32mnew file:   ML_PART/MODELS/custom_ru_core_news_lg_with_9_labels_50_epochs/ner/cfg\u001b[m\n","\t\u001b[32mnew file:   ML_PART/MODELS/custom_ru_core_news_lg_with_9_labels_50_epochs/ner/model\u001b[m\n","\t\u001b[32mnew file:   ML_PART/MODELS/custom_ru_core_news_lg_with_9_labels_50_epochs/ner/moves\u001b[m\n","\t\u001b[32mnew file:   ML_PART/MODELS/custom_ru_core_news_lg_with_9_labels_50_epochs/parser/cfg\u001b[m\n","\t\u001b[32mnew file:   ML_PART/MODELS/custom_ru_core_news_lg_with_9_labels_50_epochs/parser/model\u001b[m\n","\t\u001b[32mnew file:   ML_PART/MODELS/custom_ru_core_news_lg_with_9_labels_50_epochs/parser/moves\u001b[m\n","\t\u001b[32mnew file:   ML_PART/MODELS/custom_ru_core_news_lg_with_9_labels_50_epochs/senter/cfg\u001b[m\n","\t\u001b[32mnew file:   ML_PART/MODELS/custom_ru_core_news_lg_with_9_labels_50_epochs/senter/model\u001b[m\n","\t\u001b[32mnew file:   ML_PART/MODELS/custom_ru_core_news_lg_with_9_labels_50_epochs/tok2vec/cfg\u001b[m\n","\t\u001b[32mnew file:   ML_PART/MODELS/custom_ru_core_news_lg_with_9_labels_50_epochs/tok2vec/model\u001b[m\n","\t\u001b[32mnew file:   ML_PART/MODELS/custom_ru_core_news_lg_with_9_labels_50_epochs/tokenizer\u001b[m\n","\t\u001b[32mnew file:   ML_PART/MODELS/custom_ru_core_news_lg_with_9_labels_50_epochs/vocab/key2row\u001b[m\n","\t\u001b[32mnew file:   ML_PART/MODELS/custom_ru_core_news_lg_with_9_labels_50_epochs/vocab/strings.json\u001b[m\n","\t\u001b[32mnew file:   ML_PART/MODELS/custom_ru_core_news_lg_with_9_labels_50_epochs/vocab/vectors\u001b[m\n","\t\u001b[32mnew file:   ML_PART/MODELS/custom_ru_core_news_lg_with_9_labels_50_epochs/vocab/vectors.cfg\u001b[m\n","\t\u001b[32mnew file:   ML_PART/MODELS/custom_ru_core_news_lg_with_9_labels_9_epochs/attribute_ruler/patterns\u001b[m\n","\t\u001b[32mnew file:   ML_PART/MODELS/custom_ru_core_news_lg_with_9_labels_9_epochs/config.cfg\u001b[m\n","\t\u001b[32mnew file:   ML_PART/MODELS/custom_ru_core_news_lg_with_9_labels_9_epochs/meta.json\u001b[m\n","\t\u001b[32mnew file:   ML_PART/MODELS/custom_ru_core_news_lg_with_9_labels_9_epochs/morphologizer/cfg\u001b[m\n","\t\u001b[32mnew file:   ML_PART/MODELS/custom_ru_core_news_lg_with_9_labels_9_epochs/morphologizer/model\u001b[m\n","\t\u001b[32mnew file:   ML_PART/MODELS/custom_ru_core_news_lg_with_9_labels_9_epochs/ner/cfg\u001b[m\n","\t\u001b[32mnew file:   ML_PART/MODELS/custom_ru_core_news_lg_with_9_labels_9_epochs/ner/model\u001b[m\n","\t\u001b[32mnew file:   ML_PART/MODELS/custom_ru_core_news_lg_with_9_labels_9_epochs/ner/moves\u001b[m\n","\t\u001b[32mnew file:   ML_PART/MODELS/custom_ru_core_news_lg_with_9_labels_9_epochs/parser/cfg\u001b[m\n","\t\u001b[32mnew file:   ML_PART/MODELS/custom_ru_core_news_lg_with_9_labels_9_epochs/parser/model\u001b[m\n","\t\u001b[32mnew file:   ML_PART/MODELS/custom_ru_core_news_lg_with_9_labels_9_epochs/parser/moves\u001b[m\n","\t\u001b[32mnew file:   ML_PART/MODELS/custom_ru_core_news_lg_with_9_labels_9_epochs/senter/cfg\u001b[m\n","\t\u001b[32mnew file:   ML_PART/MODELS/custom_ru_core_news_lg_with_9_labels_9_epochs/senter/model\u001b[m\n","\t\u001b[32mnew file:   ML_PART/MODELS/custom_ru_core_news_lg_with_9_labels_9_epochs/tok2vec/cfg\u001b[m\n","\t\u001b[32mnew file:   ML_PART/MODELS/custom_ru_core_news_lg_with_9_labels_9_epochs/tok2vec/model\u001b[m\n","\t\u001b[32mnew file:   ML_PART/MODELS/custom_ru_core_news_lg_with_9_labels_9_epochs/tokenizer\u001b[m\n","\t\u001b[32mnew file:   ML_PART/MODELS/custom_ru_core_news_lg_with_9_labels_9_epochs/vocab/key2row\u001b[m\n","\t\u001b[32mnew file:   ML_PART/MODELS/custom_ru_core_news_lg_with_9_labels_9_epochs/vocab/strings.json\u001b[m\n","\t\u001b[32mnew file:   ML_PART/MODELS/custom_ru_core_news_lg_with_9_labels_9_epochs/vocab/vectors\u001b[m\n","\t\u001b[32mnew file:   ML_PART/MODELS/custom_ru_core_news_lg_with_9_labels_9_epochs/vocab/vectors.cfg\u001b[m\n","\t\u001b[32mrenamed:    MODELS_COMPARING.ipynb -> ML_PART/MODELS_COMPARING.ipynb\u001b[m\n","\t\u001b[32mnew file:   ML_PART/README.md\u001b[m\n","\t\u001b[32mrenamed:    module.py -> ML_PART/module.py\u001b[m\n","\t\u001b[32mnew file:   \"ML_PART/\\320\\245\\320\\260\\320\\272\\320\\260\\321\\202\\320\\276\\320\\275_\\321\\215\\320\\272\\321\\201\\320\\277\\320\\265\\321\\200\\320\\270\\320\\274\\320\\265\\320\\275\\321\\202\\321\\213.ipynb\"\u001b[m\n","\t\u001b[32mdeleted:    \"\\320\\245\\320\\260\\320\\272\\320\\260\\321\\202\\320\\276\\320\\275_\\321\\215\\320\\272\\321\\201\\320\\277\\320\\265\\321\\200\\320\\270\\320\\274\\320\\265\\320\\275\\321\\202\\321\\213.ipynb\"\u001b[m\n","\n","Changes not staged for commit:\n","  (use \"git add <file>...\" to update what will be committed)\n","  (use \"git restore <file>...\" to discard changes in working directory)\n","\t\u001b[31mmodified:   ML_PART/Data_explorer.ipynb\u001b[m\n","\n"]}]},{"cell_type":"code","source":["! git ls-files -c"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wQoaLDSfMr9t","executionInfo":{"status":"ok","timestamp":1759011544514,"user_tz":-180,"elapsed":134,"user":{"displayName":"Александр Федоров","userId":"11558309455709398811"}},"outputId":"b5c7c4bc-4f05-4476-f9f1-4ee844630fd3"},"execution_count":65,"outputs":[{"output_type":"stream","name":"stdout","text":[".gitignore\n","Data_explorer.ipynb\n","Fine_tuning_ru_core_web_lg.ipynb\n","MODELS_COMPARING.ipynb\n","main.py\n","module.py\n","\"\\320\\245\\320\\260\\320\\272\\320\\260\\321\\202\\320\\276\\320\\275_\\321\\215\\320\\272\\321\\201\\320\\277\\320\\265\\321\\200\\320\\270\\320\\274\\320\\265\\320\\275\\321\\202\\321\\213.ipynb\"\n"]}]},{"cell_type":"code","source":["# ! git log --oneline"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nnlD9gA5C6h2","executionInfo":{"status":"ok","timestamp":1759010946906,"user_tz":-180,"elapsed":343,"user":{"displayName":"Александр Федоров","userId":"11558309455709398811"}},"outputId":"a808a862-a65b-4539-e6bc-1082ac13e8d8"},"execution_count":55,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[33mecd5e74\u001b[m\u001b[33m (\u001b[m\u001b[1;36mHEAD -> \u001b[m\u001b[1;32mNER_models\u001b[m\u001b[33m, \u001b[m\u001b[1;31morigin/NER_models\u001b[m\u001b[33m)\u001b[m Немного доработал, поменял\n","\u001b[33mdd008a7\u001b[m Провел рефакторинг +\n","\u001b[33m4ea29b2\u001b[m Немного поменял обучение, получилось немного хуже\n","\u001b[33m1e088ca\u001b[m Вернул функцию для ссоздания оценочного  датасета\n","\u001b[33mc01f20c\u001b[m Поправил все для метрик, привел файл в более менее божеский вид\n","\u001b[33meb9d304\u001b[m Собрал все, что наисследовал в один файл\n","\u001b[33mf9fa4bd\u001b[m Сделал функцию для корректного расчета метрик, буду ее интегрировать\n","\u001b[33mc250219\u001b[m Решил откатиться и пойти по пути исследования\n","\u001b[33md8a0dfe\u001b[m Еще потуги\n","\u001b[33mbd227fa\u001b[m Поменял функцию для предобработки. Судя по всему все таки надо будет и убирать неразрывные пробелы и убирать группы пробелов и потом возвращать их на места с учетом индексации\n","\u001b[33ma360452\u001b[m Модель не учится, метрики  с ошибкой + двойная индесация. Буду фиксить\n","\u001b[33mec296c4\u001b[m Первая версия чего-то рабочего\n","\u001b[33m3130f36\u001b[m Создал файл с первым экспериментом\n","\u001b[33mf0dea78\u001b[m Добавил гитигнор\n","\u001b[33m2e562a2\u001b[m\u001b[33m (\u001b[m\u001b[1;31morigin/main\u001b[m\u001b[33m, \u001b[m\u001b[1;31morigin/HEAD\u001b[m\u001b[33m, \u001b[m\u001b[1;32mmain\u001b[m\u001b[33m)\u001b[m Сделал точку входа\n","\u001b[33mc805ddf\u001b[m Initial commit\n"]}]},{"cell_type":"code","source":["# ! git reset HEAD --mixed"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"knOYezMFDAIA","executionInfo":{"status":"ok","timestamp":1759011398962,"user_tz":-180,"elapsed":206,"user":{"displayName":"Александр Федоров","userId":"11558309455709398811"}},"outputId":"b954a5b8-61f9-418e-ca13-32dd3931e097"},"execution_count":61,"outputs":[{"output_type":"stream","name":"stdout","text":["Unstaged changes after reset:\n","D\tData_explorer.ipynb\n","D\tFine_tuning_ru_core_web_lg.ipynb\n","D\tMODELS_COMPARING.ipynb\n","D\tmodule.py\n","D\tХакатон_эксперименты.ipynb\n"]}]},{"cell_type":"code","source":["# ! git rm -r --cached ML\\ PART"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a-YjeoE7CtWk","executionInfo":{"status":"ok","timestamp":1759011191118,"user_tz":-180,"elapsed":86549,"user":{"displayName":"Александр Федоров","userId":"11558309455709398811"}},"outputId":"371452b6-abc8-4ebb-d58a-fa43af4a185e"},"execution_count":58,"outputs":[{"output_type":"stream","name":"stdout","text":["error: the following files have staged content different from both the\n","file and the HEAD:\n","    ML PART/Data_explorer.ipynb\n","    ML PART/Fine_tune_bert.ipynb\n","    ML PART/Fine_tuning_ru_core_web_lg.ipynb\n","    ML PART/MODELS/custom_ru_core_news_lg_with_9_labels_50_epochs/attribute_ruler/patterns\n","    ML PART/MODELS/custom_ru_core_news_lg_with_9_labels_50_epochs/config.cfg\n","    ML PART/MODELS/custom_ru_core_news_lg_with_9_labels_50_epochs/meta.json\n","    ML PART/MODELS/custom_ru_core_news_lg_with_9_labels_50_epochs/morphologizer/cfg\n","    ML PART/MODELS/custom_ru_core_news_lg_with_9_labels_50_epochs/morphologizer/model\n","    ML PART/MODELS/custom_ru_core_news_lg_with_9_labels_50_epochs/ner/cfg\n","    ML PART/MODELS/custom_ru_core_news_lg_with_9_labels_50_epochs/ner/model\n","    ML PART/MODELS/custom_ru_core_news_lg_with_9_labels_50_epochs/ner/moves\n","    ML PART/MODELS/custom_ru_core_news_lg_with_9_labels_50_epochs/parser/cfg\n","    ML PART/MODELS/custom_ru_core_news_lg_with_9_labels_50_epochs/parser/model\n","    ML PART/MODELS/custom_ru_core_news_lg_with_9_labels_50_epochs/parser/moves\n","    ML PART/MODELS/custom_ru_core_news_lg_with_9_labels_50_epochs/senter/cfg\n","    ML PART/MODELS/custom_ru_core_news_lg_with_9_labels_50_epochs/senter/model\n","    ML PART/MODELS/custom_ru_core_news_lg_with_9_labels_50_epochs/tok2vec/cfg\n","    ML PART/MODELS/custom_ru_core_news_lg_with_9_labels_50_epochs/tok2vec/model\n","    ML PART/MODELS/custom_ru_core_news_lg_with_9_labels_50_epochs/tokenizer\n","    ML PART/MODELS/custom_ru_core_news_lg_with_9_labels_50_epochs/vocab/key2row\n","    ML PART/MODELS/custom_ru_core_news_lg_with_9_labels_50_epochs/vocab/strings.json\n","    ML PART/MODELS/custom_ru_core_news_lg_with_9_labels_50_epochs/vocab/vectors\n","    ML PART/MODELS/custom_ru_core_news_lg_with_9_labels_50_epochs/vocab/vectors.cfg\n","    ML PART/MODELS/custom_ru_core_news_lg_with_9_labels_9_epochs/attribute_ruler/patterns\n","    ML PART/MODELS/custom_ru_core_news_lg_with_9_labels_9_epochs/config.cfg\n","    ML PART/MODELS/custom_ru_core_news_lg_with_9_labels_9_epochs/meta.json\n","    ML PART/MODELS/custom_ru_core_news_lg_with_9_labels_9_epochs/morphologizer/cfg\n","    ML PART/MODELS/custom_ru_core_news_lg_with_9_labels_9_epochs/morphologizer/model\n","    ML PART/MODELS/custom_ru_core_news_lg_with_9_labels_9_epochs/ner/cfg\n","    ML PART/MODELS/custom_ru_core_news_lg_with_9_labels_9_epochs/ner/model\n","    ML PART/MODELS/custom_ru_core_news_lg_with_9_labels_9_epochs/ner/moves\n","    ML PART/MODELS/custom_ru_core_news_lg_with_9_labels_9_epochs/parser/cfg\n","    ML PART/MODELS/custom_ru_core_news_lg_with_9_labels_9_epochs/parser/model\n","    ML PART/MODELS/custom_ru_core_news_lg_with_9_labels_9_epochs/parser/moves\n","    ML PART/MODELS/custom_ru_core_news_lg_with_9_labels_9_epochs/senter/cfg\n","    ML PART/MODELS/custom_ru_core_news_lg_with_9_labels_9_epochs/senter/model\n","    ML PART/MODELS/custom_ru_core_news_lg_with_9_labels_9_epochs/tok2vec/cfg\n","    ML PART/MODELS/custom_ru_core_news_lg_with_9_labels_9_epochs/tok2vec/model\n","    ML PART/MODELS/custom_ru_core_news_lg_with_9_labels_9_epochs/tokenizer\n","    ML PART/MODELS/custom_ru_core_news_lg_with_9_labels_9_epochs/vocab/key2row\n","    ML PART/MODELS/custom_ru_core_news_lg_with_9_labels_9_epochs/vocab/strings.json\n","    ML PART/MODELS/custom_ru_core_news_lg_with_9_labels_9_epochs/vocab/vectors\n","    ML PART/MODELS/custom_ru_core_news_lg_with_9_labels_9_epochs/vocab/vectors.cfg\n","    ML PART/MODELS_COMPARING.ipynb\n","    ML PART/README.md\n","    ML PART/module.py\n","    ML PART/Хакатон_эксперименты.ipynb\n","(use -f to force removal)\n"]}]},{"cell_type":"code","source":["! git add Data_explorer.ipynb"],"metadata":{"id":"DG2mSRjb2Q04","executionInfo":{"status":"ok","timestamp":1759012146832,"user_tz":-180,"elapsed":41770,"user":{"displayName":"Александр Федоров","userId":"11558309455709398811"}}},"execution_count":83,"outputs":[]},{"cell_type":"code","source":["! git commit -m 'Собрал все в папку, поменял ячейки для перехода в папку'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A6E6E4CS2Urg","executionInfo":{"status":"ok","timestamp":1759010635930,"user_tz":-180,"elapsed":206,"user":{"displayName":"Александр Федоров","userId":"11558309455709398811"}},"outputId":"9aa8eda4-788c-4246-ab35-9af7cdfdebac"},"execution_count":52,"outputs":[{"output_type":"stream","name":"stdout","text":["On branch NER_models\n","Your branch is up to date with 'origin/NER_models'.\n","\n","Changes not staged for commit:\n","  (use \"git add/rm <file>...\" to update what will be committed)\n","  (use \"git restore <file>...\" to discard changes in working directory)\n","\t\u001b[31mdeleted:    Data_explorer.ipynb\u001b[m\n","\t\u001b[31mdeleted:    Fine_tuning_ru_core_web_lg.ipynb\u001b[m\n","\t\u001b[31mdeleted:    MODELS_COMPARING.ipynb\u001b[m\n","\t\u001b[31mdeleted:    module.py\u001b[m\n","\t\u001b[31mdeleted:    \"\\320\\245\\320\\260\\320\\272\\320\\260\\321\\202\\320\\276\\320\\275_\\321\\215\\320\\272\\321\\201\\320\\277\\320\\265\\321\\200\\320\\270\\320\\274\\320\\265\\320\\275\\321\\202\\321\\213.ipynb\"\u001b[m\n","\n","Untracked files:\n","  (use \"git add <file>...\" to include in what will be committed)\n","\t\u001b[31mML PART/\u001b[m\n","\n","no changes added to commit (use \"git add\" and/or \"git commit -a\")\n"]}]},{"cell_type":"code","source":["! git push"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QD6mmAJj2VNY","executionInfo":{"status":"ok","timestamp":1759010294272,"user_tz":-180,"elapsed":24856,"user":{"displayName":"Александр Федоров","userId":"11558309455709398811"}},"outputId":"bd0317de-53f7-4690-9653-2be8fd042870"},"execution_count":45,"outputs":[{"output_type":"stream","name":"stdout","text":["Enumerating objects: 48, done.\n","Counting objects:   2% (1/48)\rCounting objects:   4% (2/48)\rCounting objects:   6% (3/48)\rCounting objects:   8% (4/48)\rCounting objects:  10% (5/48)\rCounting objects:  12% (6/48)\rCounting objects:  14% (7/48)\rCounting objects:  16% (8/48)\rCounting objects:  18% (9/48)\rCounting objects:  20% (10/48)\rCounting objects:  22% (11/48)\rCounting objects:  25% (12/48)\rCounting objects:  27% (13/48)\rCounting objects:  29% (14/48)\rCounting objects:  31% (15/48)\rCounting objects:  33% (16/48)\rCounting objects:  35% (17/48)\rCounting objects:  37% (18/48)\rCounting objects:  39% (19/48)\rCounting objects:  41% (20/48)\rCounting objects:  43% (21/48)\rCounting objects:  45% (22/48)\rCounting objects:  47% (23/48)\rCounting objects:  50% (24/48)\rCounting objects:  52% (25/48)\rCounting objects:  54% (26/48)\rCounting objects:  56% (27/48)\rCounting objects:  58% (28/48)\rCounting objects:  60% (29/48)\rCounting objects:  62% (30/48)\rCounting objects:  64% (31/48)\rCounting objects:  66% (32/48)\rCounting objects:  68% (33/48)\rCounting objects:  70% (34/48)\rCounting objects:  72% (35/48)\rCounting objects:  75% (36/48)\rCounting objects:  77% (37/48)\rCounting objects:  79% (38/48)\rCounting objects:  81% (39/48)\rCounting objects:  83% (40/48)\rCounting objects:  85% (41/48)\rCounting objects:  87% (42/48)\rCounting objects:  89% (43/48)\rCounting objects:  91% (44/48)\rCounting objects:  93% (45/48)\rCounting objects:  95% (46/48)\rCounting objects:  97% (47/48)\rCounting objects: 100% (48/48)\rCounting objects: 100% (48/48), done.\n","Delta compression using up to 2 threads\n","Compressing objects: 100% (42/42), done.\n","Writing objects: 100% (47/47), 491.34 MiB | 29.53 MiB/s, done.\n","Total 47 (delta 9), reused 2 (delta 0), pack-reused 0\n","remote: Resolving deltas: 100% (9/9), completed with 1 local object.\u001b[K\n","remote: \u001b[1;31merror\u001b[m: Trace: f2b69bbaa3429a3593e3ef2782eec4b24776078723672e36c217d35e1191abeb\u001b[K\n","remote: \u001b[1;31merror\u001b[m: See https://gh.io/lfs for more information.\u001b[K\n","remote: \u001b[1;31merror\u001b[m: File ML PART/MODELS/custom_ru_core_news_lg_with_9_labels_50_epochs/vocab/vectors is 572.21 MB; this exceeds GitHub's file size limit of 100.00 MB\u001b[K\n","remote: \u001b[1;31merror\u001b[m: GH001: Large files detected. You may want to try Git Large File Storage - https://git-lfs.github.com.\u001b[K\n","To https://github.com/tokarevdr/entities-extraction-x5.git\n"," \u001b[31m! [remote rejected]\u001b[m NER_models -> NER_models (pre-receive hook declined)\n","\u001b[31merror: failed to push some refs to 'https://github.com/tokarevdr/entities-extraction-x5.git'\n","\u001b[m"]}]},{"cell_type":"code","source":["! pip install pyspellchecker"],"metadata":{"id":"uo7M5V7i6A_j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import ast\n","import matplotlib.pyplot as plt\n","from collections import Counter, defaultdict\n","from sklearn.model_selection import train_test_split\n","import seaborn as sns\n","from sklearn.preprocessing import MultiLabelBinarizer\n","import numpy as np\n","import re\n","from typing import Set, List, Tuple, Any"],"metadata":{"id":"i3Uq0Wnm2W9Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Извлекаем уникальные метки для каждой строки (игнорируя B/I/O, фокусируемся на TYPE, PERCENT и т.д.)\n","def get_entity_types(annotation) -> Set[str]:\n","    if not annotation:\n","        return {'O'}  # Если аннотаций нет, считаем как только O\n","    types = set()\n","    for start, end, label in annotation:\n","        entity_type = label.split('-')[-1]  # 'B-TYPE' -> 'TYPE', 'I-PERCENT' -> 'PERCENT'\n","        if entity_type != 'O':\n","            types.add(entity_type)\n","    if not types:\n","        types.add('O')\n","    return types"],"metadata":{"id":"oAUOseYV2fBr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Удаление спецсимволов"],"metadata":{"id":"tQsg3Eofhybs"}},{"cell_type":"code","source":["def normalize_whitespace_characters(df: pd.DataFrame, text_column='sample') -> pd.DataFrame:\n","    \"\"\"\n","    Заменяет различные пробельные символы UTF-8 на обычный пробел.\n","    Возвращает модифицированный DataFrame и выводит статистику замен.\n","    \"\"\"\n","    # Словарь пробельных символов UTF-8 для замены\n","    whitespace_characters = {\n","        'Ogham Space Mark': '\\u1680',\n","        'Mongolian Vowel Separator': '\\u180E',\n","        'En Quad': '\\u2000',\n","        'Em Quad': '\\u2001',\n","        'En Space': '\\u2002',\n","        'Em Space': '\\u2003',\n","        'Three-Per-Em Space': '\\u2004',\n","        'Four-Per-Em Space': '\\u2005',\n","        'Six-Per-Em Space': '\\u2006',\n","        'Figure Space': '\\u2007',\n","        'Punctuation Space': '\\u2008',\n","        'Thin Space': '\\u2009',\n","        'Hair Space': '\\u200A',\n","        'Zero Width Space': '\\u200B',\n","        'Narrow No-Break Space': '\\u202F',\n","        'Medium Mathematical Space': '\\u205F',\n","        'Ideographic Space': '\\u3000',\n","        'Zero Width No-Break Space': '\\uFEFF'\n","    }\n","\n","    # Также включаем неразрывный пробел, который часто встречается\n","    whitespace_characters['Non-breaking Space'] = '\\u00A0'\n","\n","    replacement_stats = Counter()\n","    total_replacements = 0\n","\n","    def replace_special_spaces(text):\n","        nonlocal total_replacements\n","        if not isinstance(text, str):\n","            return text\n","\n","        original_text = text\n","        for char_name, char_code in whitespace_characters.items():\n","            if char_code in text:\n","                count = text.count(char_code)\n","                replacement_stats[char_name] += count\n","                total_replacements += count\n","                text = text.replace(char_code, ' ')  # Заменяем на обычный пробел\n","\n","        return text\n","\n","    # Применяем замену к указанной колонке\n","    df_modified = df.copy()\n","    df_modified[text_column] = df_modified[text_column].apply(replace_special_spaces)\n","\n","    # Выводим статистику\n","    print(\"Статистика замен пробельных символов:\")\n","    print(\"-\" * 50)\n","    if total_replacements == 0:\n","        print(\"Не найдено специальных пробельных символов для замены.\")\n","    else:\n","        for char_name, count in replacement_stats.most_common():\n","            print(f\"{char_name}: {count} замен\")\n","        print(\"-\" * 50)\n","        print(f\"Всего замен: {total_replacements}\")\n","\n","    return df_modified"],"metadata":{"id":"4py1Ditlh0rs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = normalize_whitespace_characters(df)"],"metadata":{"id":"ugRgIMnOh1SK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Исследование датасета"],"metadata":{"id":"qQ6UdFbviiHO"}},{"cell_type":"code","source":["def plot_sample_length_distribution(df: pd.DataFrame, name: str=None, bins: int=30):\n","    \"\"\"\n","    Строит гистограмму распределения длин строк\n","    \"\"\"\n","    lengths = df['sample'].apply(len)\n","    plt.figure(figsize=(10, 6))\n","    plt.hist(lengths, bins=bins,  edgecolor='black')\n","    plt.title('Распределение длин строк')\n","    plt.xlabel('Длина строки')\n","    plt.ylabel('Количество строк')\n","    plt.grid(True)\n","    if name:\n","      plt.savefig(fname=f'Распределение длин строк в {name}')\n","    plt.show()\n","    plt.close()"],"metadata":{"id":"E_qrEzfGigZR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plot_entity_distributions(df):\n","    \"\"\"\n","    Строит графики распределения сущностей по строкам\n","    \"\"\"\n","    # Input validation\n","    if not isinstance(df, pd.DataFrame):\n","        raise ValueError(\"Input must be a pandas DataFrame\")\n","    if 'entity_types' not in df.columns:\n","        raise ValueError(\"DataFrame must contain 'entity_types' column\")\n","    if df.empty:\n","        raise ValueError(\"DataFrame is empty\")\n","\n","    # Total number of rows in the dataset\n","    total_rows = len(df)\n","\n","    # Count rows with exactly one entity type\n","    only_single = defaultdict(int)\n","    for types in df['entity_types']:\n","        if isinstance(types, (list, set, tuple)) and len(types) == 1:\n","            entity = list(types)[0]\n","            if entity and isinstance(entity, str):\n","                only_single[entity] += 1\n","\n","    # Count rows containing each entity type (including combinations)\n","    contains_each = Counter()\n","    for types in df['entity_types']:\n","        if isinstance(types, (list, set, tuple)):\n","            for t in types:\n","                if t and isinstance(t, str):\n","                    contains_each[t] += 1\n","\n","    # Prepare data for visualization\n","    labels_single = list(only_single.keys())\n","    counts_single = list(only_single.values())\n","    labels_contains = list(contains_each.keys())\n","    counts_contains = list(contains_each.values())\n","\n","    # Calculate percentages relative to total dataset size\n","    percent_single = [c / total_rows * 100 for c in counts_single]\n","    percent_contains = [c / total_rows * 100 for c in counts_contains]\n","\n","    # Visualization\n","    fig, ax = plt.subplots(2, 1, figsize=(10, 10))\n","\n","    # Top plot: Rows with exactly one entity type\n","    bars0 = ax[0].bar(labels_single, counts_single, color='skyblue')\n","    ax[0].set_title('Распределение строк с ровно одной сущностью')\n","    ax[0].set_ylabel('Количество')\n","    max_count0 = max(counts_single) if counts_single else 1\n","    for i, bar in enumerate(bars0):\n","        height = bar.get_height()\n","        ax[0].text(bar.get_x() + bar.get_width()/2., height + 0.01 * max_count0,\n","                   f'{int(height)} ({percent_single[i]:.1f}%)', ha='center', va='bottom', fontsize=10)\n","\n","    # Bottom plot: Rows containing each entity type\n","    bars1 = ax[1].bar(labels_contains, counts_contains, color='lightgreen')\n","    ax[1].set_title('Распределение строк, содержащих каждую сущность')\n","    ax[1].set_ylabel('Количество')\n","    max_count1 = max(counts_contains) if counts_contains else 1\n","    for i, bar in enumerate(bars1):\n","        height = bar.get_height()\n","        ax[1].text(bar.get_x() + bar.get_width()/2., height + 0.01 * max_count1,\n","                   f'{int(height)} ({percent_contains[i]:.1f}%)', ha='center', va='bottom', fontsize=10)\n","\n","    plt.tight_layout()\n","    plt.show()\n","    plt.close()\n","\n","    # Console output for verification\n","    print(\"Строки с ровно одной сущностью:\")\n","    for label, count in only_single.items():\n","        print(f\"{label}: {count} строк ({count / total_rows * 100:.1f}%)\")\n","    print(\"\\nСтроки, содержащие каждую сущность (включая комбинации):\")\n","    for label, count in contains_each.items():\n","        print(f\"{label}: {count} строк ({count / total_rows * 100:.1f}%)\")"],"metadata":{"id":"bph2rgh2imxV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plot_entity_frequency(df):\n","    \"\"\"\n","    Строит график частоты сущностей по типам\n","    \"\"\"\n","    all_labels = []\n","    for ann in df['annotation_parsed']:\n","        for _, _, label in ann:\n","            entity_type = label.split('-')[-1]\n","            all_labels.append(entity_type)\n","\n","    entity_freq = Counter(all_labels)\n","    print(\"\\nРаспределение частоты сущностей:\")\n","    print(entity_freq)\n","\n","    plt.figure(figsize=(8, 5))\n","    plt.bar(entity_freq.keys(), entity_freq.values(), color='purple')\n","    plt.title('Частота сущностей по типам')\n","    plt.xlabel('Тип сущности')\n","    plt.ylabel('Количество')\n","    plt.grid(True)\n","    plt.show()\n","    plt.close()"],"metadata":{"id":"WbhljUsqi0Po"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plot_span_length_distribution(df):\n","    \"\"\"\n","    Строит гистограмму распределения длин спанов сущностей\n","    \"\"\"\n","    span_lengths = []\n","    for ann in df['annotation_parsed']:\n","        for start, end, _ in ann:\n","            span_lengths.append(end - start)\n","\n","    if span_lengths:\n","        plt.figure(figsize=(10, 6))\n","        plt.hist(span_lengths, bins=20, edgecolor='black')\n","        plt.title('Распределение длин спанов сущностей')\n","        plt.xlabel('Длина спана')\n","        plt.ylabel('Количество')\n","        plt.grid(True)\n","        plt.show()\n","        plt.close()\n","    else:\n","        print(\"Нет аннотаций для расчёта длин спанов.\")"],"metadata":{"id":"Jx_M1ZAdi3Ka"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plot_entities_per_sample(df):\n","    \"\"\"\n","    Строит гистограмму распределения количества сущностей на строку\n","    \"\"\"\n","    num_entities_per_sample = df['annotation_parsed'].apply(len)\n","    plt.figure(figsize=(10, 6))\n","    plt.hist(num_entities_per_sample, bins=range(num_entities_per_sample.max() + 2), edgecolor='black')\n","    plt.title('Распределение количества сущностей на строку')\n","    plt.xlabel('Количество сущностей')\n","    plt.ylabel('Количество строк')\n","    plt.grid(True)\n","    plt.show()\n","    plt.close()"],"metadata":{"id":"JmvIE5gYi51I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def analyze_bio_distribution(df):\n","    \"\"\"\n","    Анализирует распределение BIO-меток\n","    \"\"\"\n","    bio_counter = defaultdict(Counter)\n","    for ann in df['annotation_parsed']:\n","        for _, _, label in ann:\n","            bio_prefix = label.split('-')[0]\n","            entity_type = label.split('-')[-1]\n","            bio_counter[entity_type][bio_prefix] += 1\n","\n","    print(\"\\nРаспределение BIO по меткам:\")\n","    for entity_type, counts in bio_counter.items():\n","        print(f\"{entity_type}: B={counts['B']}, I={counts['I']}\")\n","\n","    # Проверка на I без B\n","    for entity_type, counts in bio_counter.items():\n","        if counts['I'] > 0 and counts['B'] == 0:\n","            print(f\"Предупреждение: Для {entity_type} есть I без B — возможная ошибка в данных.\")"],"metadata":{"id":"uLM-Erj1i8xI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def analyze_spelling(df):\n","    \"\"\"\n","    Анализирует опечатки и частоту уникальных слов\n","    \"\"\"\n","    unique_words_freq = df['sample'].str.split().explode().value_counts()\n","    print(\"\\nЧастота уникальных слов (топ-20):\")\n","    print(unique_words_freq.head(20))\n","\n","    # Spell checking\n","    try:\n","        from spellchecker import SpellChecker\n","        spell = SpellChecker(language='ru')\n","        misspelled = set()\n","        for word in unique_words_freq.index:\n","            if word not in spell:\n","                misspelled.add(word)\n","        print(\"\\nПотенциальные опечатки (по pyspellchecker, топ-20):\")\n","        print(list(misspelled)[:20])\n","    except ImportError:\n","        print(\"pyspellchecker не установлен; пропускаем проверку опечаток.\")"],"metadata":{"id":"atdaE4Eji_7E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def find_overlapping_spans(df):\n","    \"\"\"\n","    Находит пересекающиеся спаны сущностей\n","    \"\"\"\n","    overlaps = 0\n","    for idx, ann in enumerate(df['annotation_parsed']):\n","        spans = sorted([(start, end) for start, end, _ in ann])\n","        for i in range(1, len(spans)):\n","            if spans[i-1][1] > spans[i][0]:\n","                overlaps += 1\n","                print(f\"Пересечение в строке {idx}: {spans[i-1]} и {spans[i]}\")\n","    if overlaps == 0:\n","        print(\"\\nНет пересекающихся спанов.\")\n","    else:\n","        print(f\"\\nОбнаружено {overlaps} пересекающихся спанов — проверьте данные!\")"],"metadata":{"id":"mGUC9CwtjCKa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plot_entity_correlations(df):\n","    \"\"\"\n","    Строит тепловую карту корреляций между метками\n","    \"\"\"\n","    mlb = MultiLabelBinarizer()\n","    entity_matrix = mlb.fit_transform(df['entity_types'])\n","    corr_matrix = pd.DataFrame(entity_matrix, columns=mlb.classes_).corr()\n","\n","    plt.figure(figsize=(8, 6))\n","    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n","    plt.title('Корреляции между метками')\n","    plt.show()\n","    plt.close()"],"metadata":{"id":"PlkaOUS7jFHh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def iterative_train_test_split(X, y, test_size=0.2, random_state=42):\n","    \"\"\"\n","    Кастомная функция для multi-label stratified split\n","    \"\"\"\n","    np.random.seed(random_state)\n","    n_samples, n_labels = y.shape\n","    n_test = int(n_samples * test_size)\n","    n_train = n_samples - n_test\n","\n","    label_counts = y.sum(axis=0)\n","    sorted_labels = np.argsort(label_counts)[::-1]\n","\n","    train_idx = set()\n","    test_idx = set()\n","    assigned = np.zeros(n_samples, dtype=bool)\n","\n","    for label in sorted_labels:\n","        samples_with_label = np.where(y[:, label] == 1)[0]\n","        unassigned_with_label = samples_with_label[~assigned[samples_with_label]]\n","\n","        if len(unassigned_with_label) == 0:\n","            continue\n","\n","        all_with_label = len(samples_with_label)\n","        desired_test = int(np.ceil(all_with_label * test_size))\n","        current_test = sum(1 for i in samples_with_label if i in test_idx)\n","        to_add_test = max(0, desired_test - current_test)\n","\n","        np.random.shuffle(unassigned_with_label)\n","        to_test = unassigned_with_label[:to_add_test]\n","        test_idx.update(to_test)\n","        assigned[to_test] = True\n","\n","        to_train = unassigned_with_label[to_add_test:]\n","        train_idx.update(to_train)\n","        assigned[to_train] = True\n","\n","    unassigned = np.where(~assigned)[0]\n","    train_idx.update(unassigned)\n","\n","    train_idx = np.array(list(train_idx))\n","    test_idx = np.array(list(test_idx))\n","    train_idx.sort()\n","    test_idx.sort()\n","\n","    return X.iloc[train_idx], X.iloc[test_idx], y[train_idx], y[test_idx]"],"metadata":{"id":"duzToQcFjRE2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def split_data_stratified(df, test_size=0.2, val_size=0.15, random_state=42):\n","    \"\"\"\n","    Разделяет данные на train, validation и test с сохранением распределения меток\n","    \"\"\"\n","    mlb = MultiLabelBinarizer()\n","    y = mlb.fit_transform(df['entity_types'])\n","    labels = mlb.classes_\n","\n","    print(\"\\nМетки в датасете:\", labels)\n","\n","    full_proportions = {label: y[:, i].mean() for i, label in enumerate(labels)}\n","    print(\"\\nПропорции меток в полном датасете:\")\n","    for label, prop in full_proportions.items():\n","        print(f\"{label}: {prop:.4f} ({int(y[:, labels.tolist().index(label)].sum())} строк)\")\n","\n","    # Разделение на train и temp\n","    df_train, df_temp, y_train, y_temp = iterative_train_test_split(df, y, test_size=test_size+val_size, random_state=random_state)\n","\n","    # Разделение temp на val и test\n","    temp_test_size = val_size / (test_size + val_size)\n","    df_val, df_test, y_val, y_test = iterative_train_test_split(df_temp, y_temp, test_size=temp_test_size, random_state=random_state)\n","\n","    # Сохраняем в файлы\n","    train_df = df_train.drop(columns=['annotation_parsed', 'entity_types', 'group'], errors='ignore')\n","    val_df = df_val.drop(columns=['annotation_parsed', 'entity_types', 'group'], errors='ignore')\n","    test_df = df_test.drop(columns=['annotation_parsed', 'entity_types', 'group'], errors='ignore')\n","\n","    def get_proportions(y_matrix, labels):\n","        return {label: y_matrix[:, i].mean() for i, label in enumerate(labels)}\n","\n","    print(\"\\nПропорции меток в train:\")\n","    print(get_proportions(y_train, labels))\n","    print(\"\\nПропорции меток в val:\")\n","    print(get_proportions(y_val, labels))\n","    print(\"\\nПропорции меток в test:\")\n","    print(get_proportions(y_test, labels))\n","\n","    return train_df, val_df, test_df"],"metadata":{"id":"uWKq-9ZZjRou"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plot_entity_percentages(df):\n","    \"\"\"\n","    Строит круговую диаграмму процентного соотношения сущностей\n","    \"\"\"\n","    if not isinstance(df, pd.DataFrame):\n","        raise ValueError(\"Input must be a pandas DataFrame\")\n","    if 'entity_types' not in df.columns:\n","        raise ValueError(\"DataFrame must contain 'entity_types' column\")\n","    if df.empty:\n","        raise ValueError(\"DataFrame is empty\")\n","\n","    label_counts = Counter()\n","    for types in df['entity_types']:\n","        if isinstance(types, (list, set, tuple)):\n","            for t in types:\n","                if t:\n","                    label_counts[t] += 1\n","\n","    if not label_counts:\n","        raise ValueError(\"No valid entity types found in 'entity_types' column\")\n","\n","    total_samples = len(df)\n","    label_percentages = {label: (count / total_samples * 100) for label, count in label_counts.items()}\n","\n","    print(\"\\nPercentage of rows containing each entity type:\")\n","    for label, percentage in label_percentages.items():\n","        print(f\"{label}: {percentage:.2f}% ({label_counts[label]} rows)\")\n","    print('-' * 50)\n","\n","    plt.figure(figsize=(8, 8))\n","    plt.pie(\n","        [percentage for label, percentage in label_percentages.items()],\n","        labels=[label for label, percentage in label_percentages.items()],\n","        colors=plt.cm.Paired(range(len(label_percentages))),\n","        startangle=90,\n","        autopct='%1.1f%%',\n","        textprops={'fontsize': 12}\n","    )\n","    plt.title('Percentage of Rows by Entity Type', fontsize=14)\n","    plt.axis('equal')\n","    plt.show()\n","    plt.close()"],"metadata":{"id":"LJNyjPO2jUdi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"tZXzcpGyqthL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Загрузка данного набора данных"],"metadata":{"id":"n93Xk8Mxqdc6"}},{"cell_type":"code","source":["df = pd.read_csv('train.csv', sep=';', encoding='utf-8')"],"metadata":{"id":"bNkS3Yfpqbz3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['annotation_parsed'] = df['annotation'].apply(ast.literal_eval)"],"metadata":{"id":"3euyTneTqevw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.head(10)"],"metadata":{"id":"4CMRgG-sqmtg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['entity_types'] = df['annotation_parsed'].apply(get_entity_types)"],"metadata":{"id":"2X56fmpLqoO8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(df.sample(10))"],"metadata":{"id":"z0ipNgeJoDqt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Выполнение всех анализов\n","print(\"\\n=== АНАЛИЗ ДАННЫХ ===\")\n","plot_sample_length_distribution(df)\n","\n"],"metadata":{"id":"PYpZUatvCUEP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_entity_distributions(df)\n"],"metadata":{"id":"D2PRpBRynEV1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_entity_frequency(df)\n"],"metadata":{"id":"1osal8oCn2Fu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_span_length_distribution(df)\n"],"metadata":{"id":"E4Fiu1A4n3Qo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_entities_per_sample(df)\n"],"metadata":{"id":"r434_7Xan4UP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["analyze_bio_distribution(df)"],"metadata":{"id":"prNJWHavn5Yn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","analyze_spelling(df)\n"],"metadata":{"id":"UQAZCVwPn6ow"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["find_overlapping_spans(df)\n"],"metadata":{"id":"gXqW2Llvn7QK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_entity_correlations(df)\n"],"metadata":{"id":"7ZiI3N0-n8__"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["path = os.getwc()+'/OUTPUT/train_cleared.csv'\n","df.to_csv(path)"],"metadata":{"id":"eJlidVekt0ed"},"execution_count":null,"outputs":[]}]}