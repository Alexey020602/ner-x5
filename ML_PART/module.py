# -*- coding: utf-8 -*-
"""module.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WrgO_TQ_l8oS-6Ffl4L66MqY0JXYg7Bo

Файл для всяких функций, которые везде буду использовать
"""

def calculate_ner_metrics(true_entities, pred_entities):
    """
    Calculate TP, FP, FN for each entity type (TYPE, BRAND, VOLUME, PERCENT) based on BIO tagging.

    Args:
        true_entities: List of tuples [(start, end, label), ...] for ground truth.
        pred_entities: List of tuples [(start, end, label), ...] for predictions.

    Returns:
        Dict with TP, FP, FN counts for each entity type.
    """
    # Define entity types
    entity_types = ['TYPE', 'BRAND', 'VOLUME', 'PERCENT']

    # Initialize result dictionary
    metrics = {entity: {'TP': 0, 'FP': 0, 'FN': 0} for entity in entity_types}

    # Optimized grouping function
    def group_entities(entities):
        grouped = {entity: [] for entity in entity_types}

        # Filter and pre-process entities
        valid_entities = []
        for start, end, label in entities:
            if '-' in label:
                prefix, entity_type = label.split('-', 1)
                if entity_type in entity_types:
                    valid_entities.append((start, end, prefix, entity_type))

        # Sort by start position only once
        valid_entities.sort(key=lambda x: x[0])

        # Group entities
        current_entity = None
        current_type = None
        current_start = None
        current_end = None

        for start, end, prefix, entity_type in valid_entities:
            if prefix == 'B':
                # Save previous entity
                if current_entity is not None:
                    grouped[current_type].append((current_start, current_end))

                # Start new entity
                current_entity = entity_type
                current_type = entity_type
                current_start = start
                current_end = end
            elif prefix == 'I' and current_entity == entity_type:
                # Continue current entity
                current_end = end
            else:
                # Save previous entity and reset
                if current_entity is not None:
                    grouped[current_type].append((current_start, current_end))
                current_entity = None
                current_type = None
                current_start = None
                current_end = None

        # Save last entity
        if current_entity is not None:
            grouped[current_type].append((current_start, current_end))

        return grouped

    # Group true and predicted entities
    true_grouped = group_entities(true_entities)
    pred_grouped = group_entities(pred_entities)

    # Calculate TP, FP, FN for each entity type using sets for faster operations
    for entity_type in entity_types:
        # Convert to sets of tuples for faster set operations
        true_spans = set(true_grouped[entity_type])
        pred_spans = set(pred_grouped[entity_type])

        # True Positives: spans that are in both true and predicted
        metrics[entity_type]['TP'] = len(true_spans & pred_spans)

        # False Positives: spans in predicted but not in true
        metrics[entity_type]['FP'] = len(pred_spans - true_spans)

        # False Negatives: spans in true but not in predicted
        metrics[entity_type]['FN'] = len(true_spans - pred_spans)

    return metrics

def calculate_macro_f1(entity_pairs, max_processing_time=1.0):
    """
    Calculate macro-averaged F1-score and individual F1-scores for each entity type.

    Args:
        entity_pairs: List of tuples [(true_entities, pred_entities), ...] where each true_entities
                      and pred_entities is a list of tuples [(start, end, label), ...].
        max_processing_time: Maximum allowed processing time per pair in seconds (default: 1.0).

    Returns:
        Tuple: (macro_f1, f1_type, f1_brand, f1_volume, f1_percent)
            - macro_f1: Macro-averaged F1-score across active entity types.
            - f1_type: F1-score for TYPE entity.
            - f1_brand: F1-score for BRAND entity.
            - f1_volume: F1-score for VOLUME entity.
            - f1_percent: F1-score for PERCENT entity.
    """
    # Use all entity types including PERCENT
    entity_types = ['TYPE', 'BRAND', 'VOLUME', 'PERCENT']
    total_metrics = {entity: {'TP': 0, 'FP': 0, 'FN': 0} for entity in entity_types}

    # Process each pair sequentially (keeping the same order for output)
    results = []
    for i, pair in enumerate(entity_pairs, start=1):
        metrics = calculate_ner_metrics(pair[0], pair[1])
        results.append(metrics)

    # Aggregate TP, FP, FN across all pairs
    for metrics in results:
        for entity_type in entity_types:
            total_metrics[entity_type]['TP'] += metrics[entity_type]['TP']
            total_metrics[entity_type]['FP'] += metrics[entity_type]['FP']
            total_metrics[entity_type]['FN'] += metrics[entity_type]['FN']

    # Calculate Precision, Recall, F1 for each entity type
    f1_scores = []
    f1_per_entity = {entity: 0.0 for entity in entity_types}  # Store F1 for each entity type

    for entity_type in entity_types:
        tp = total_metrics[entity_type]['TP']
        fp = total_metrics[entity_type]['FP']
        fn = total_metrics[entity_type]['FN']

        # Calculate Precision
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0

        # Calculate Recall
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0

        # Calculate F1
        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0

        # Store F1 for this entity type
        f1_per_entity[entity_type] = f1

        # Добавляем F1 в список только для типов, где есть хотя бы одна сущность (TP+FP+FN > 0)
        if tp + fp + fn > 0:
            f1_scores.append(f1)

    # Calculate macro-averaged F1-score
    macro_f1 = sum(f1_scores) / len(f1_scores) if f1_scores else 0.0

    # Return macro_f1 and individual F1 scores for each entity type
    return (macro_f1,
            f1_per_entity['TYPE'],
            f1_per_entity['BRAND'],
            f1_per_entity['VOLUME'],
            f1_per_entity['PERCENT'])

def evaluate_model(model, eval_data):
    """Вычисление метрик на валидационной выборке"""
    entity_pairs = []

    for text, annotations in eval_data:
        # Получаем предсказания модели
        doc = model(text)
        pred_entities = [(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]

        # Истинные сущности
        true_entities = annotations['entities']

        entity_pairs.append((true_entities, pred_entities))

    # Вычисляем метрики
    macro_f1, f1_type, f1_brand, f1_volume, f1_percent = calculate_macro_f1(entity_pairs)

    return {
        'f1_macro': macro_f1,
        'f1_TYPE': f1_type,
        'f1_BRAND': f1_brand,
        'f1_VOLUME': f1_volume,
        'f1_PERCENT': f1_percent
    }

import pandas as pd

def process_submission(trained_model, input_file='submission.csv', output_file='submission_response.csv'):
    """
    Обрабатывает файл с примерами через модель и сохраняет результаты в новый файл.

    Args:
        trained_model: Ваша обученная модель для распознавания сущностей
        input_file (str): Путь к входному файлу
        output_file (str): Путь к выходному файлу
    """

    # Читаем исходный файл
    df = pd.read_csv(input_file, sep=';')

    results = []

    for text in df['sample']:
        # Прогоняем текст через модель
        doc = trained_model(text)

        # Извлекаем сущности в нужном формате
        entities = [(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]

        results.append(entities)

    # Создаем новый DataFrame с результатами
    output_df = pd.DataFrame({
        'sample': df['sample'],
        'annotation': results
    })

    # Сохраняем в CSV с правильным форматом
    output_df.to_csv(output_file, sep=';', index=False)

class HFWrapper:
    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer

    def __call__(self, text):
        class Doc:
            def __init__(self, ents):
                self.ents = ents

        class Ent:
            def __init__(self, start, end, label):
                self.start_char = start
                self.end_char = end
                self.label_ = label

        tokenized = self.tokenizer([text], padding=True, truncation=True, return_tensors="pt", return_offsets_mapping=True)
        input_ids = tokenized["input_ids"].to(self.model.bert.device)
        attention_mask = tokenized["attention_mask"].to(self.model.bert.device)
        with torch.no_grad():
            pred = self.model(input_ids, attention_mask)[0]
        spans = bio_to_spans(text, pred, tokenized["offset_mapping"][0].tolist())
        ents = [Ent(s, e, l) for s, e, l in spans]
        return Doc(ents)